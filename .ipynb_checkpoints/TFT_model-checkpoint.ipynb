{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76876d8-aaa7-4c88-bc1b-aeefd6e599e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch Forecasting 관련 라이브러리\n",
    "import torch\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, MAE\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9150cf34-0d17-4404-a5c2-48c54146d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로딩 중...\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "print(\"데이터 로딩 중...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 날짜 컬럼 datetime 형태로 변환\n",
    "train_df['일시'] = pd.to_datetime(train_df['일시'])\n",
    "test_df['일시'] = pd.to_datetime(test_df['일시'])\n",
    "\n",
    "# 건물번호 str타입으로 변환\n",
    "train_df['건물번호'] = train_df['건물번호'].astype(str)\n",
    "test_df['건물번호'] = test_df['건물번호'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eceb3db1-e283-4a89-b695-bc167013fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 생성 중...\n",
      "학습 데이터: 115200 샘플\n",
      "검증 데이터: 88800 샘플\n"
     ]
    }
   ],
   "source": [
    "# 시간 관련 특성 생성\n",
    "def create_time_features(df):\n",
    "    df['hour'] = df['일시'].dt.hour\n",
    "    df['day'] = df['일시'].dt.day\n",
    "    df['month'] = df['일시'].dt.month\n",
    "    df['weekday'] = df['일시'].dt.weekday\n",
    "    df['day_of_year'] = df['일시'].dt.dayofyear\n",
    "    \n",
    "    # 주말 여부\n",
    "    df['is_weekend'] = (df['weekday'] >= 5).astype(str)\n",
    "    \n",
    "    # 시간대별 구분 (새벽, 아침, 낮, 저녁, 밤)\n",
    "    df['time_segment'] = pd.cut(df['hour'], \n",
    "                                bins=[-1, 6, 9, 12, 18, 21, 24],\n",
    "                                labels=['dawn', 'morning', 'noon', 'afternoon', 'evening', 'night'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 특성 생성\n",
    "print(\"특성 생성 중...\")\n",
    "train_df = create_time_features(train_df)\n",
    "test_df = create_time_features(test_df)\n",
    "\n",
    "# 건물별로 그룹화하여 시계열 순서 확인\n",
    "train_df = train_df.sort_values(['건물번호', '일시']).reset_index(drop=True)\n",
    "\n",
    "# TFT를 위한 시계열 인덱스 생성\n",
    "train_df['time_idx'] = train_df.groupby('건물번호').cumcount()\n",
    "\n",
    "# 전력사용량 로그 변환 (분포 정규화)\n",
    "train_df['log_전력사용량'] = np.log1p(train_df['전력소비량(kWh)'])\n",
    "\n",
    "# 학습/검증 데이터 분할\n",
    "# 30일 인코더 + 7일 예측 = 37일\n",
    "validation_cutoff = train_df['일시'].max() - timedelta(days=37)\n",
    "training = train_df[train_df['일시'] <= validation_cutoff]\n",
    "validation = train_df[train_df['일시'] > validation_cutoff]\n",
    "\n",
    "print(f\"학습 데이터: {len(training)} 샘플\")\n",
    "print(f\"검증 데이터: {len(validation)} 샘플\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc270af5-4871-4a47-a607-996b43aab42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesDataSet 생성\n",
    "max_prediction_length = 24 * 7  # 7일 예측\n",
    "max_encoder_length = 24 * 30   # 30일 과거 데이터 사용\n",
    "\n",
    "# 특성 설정\n",
    "time_varying_known_reals = ['hour', 'day', 'month', 'weekday', 'day_of_year',\n",
    "                           '기온(°C)', '풍속(m/s)', '습도(%)', '강수량(mm)', \n",
    "                           '일조(hr)', '일사(MJ/m2)']\n",
    "\n",
    "time_varying_unknown_reals = ['log_전력사용량']\n",
    "\n",
    "static_categoricals = ['건물번호']\n",
    "\n",
    "time_varying_known_categoricals = ['is_weekend', 'time_segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34208341-ad54-4604-9e99-ecf3ea80e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "training_dataset = TimeSeriesDataSet(\n",
    "    training,\n",
    "    time_idx='time_idx',\n",
    "    target='log_전력사용량',\n",
    "    group_ids=['건물번호'],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=static_categoricals,\n",
    "    time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['건물번호'],\n",
    "        transformation=\"softplus\"\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "validation_dataset = TimeSeriesDataSet.from_dataset(\n",
    "    training_dataset, \n",
    "    validation, \n",
    "    predict=True, \n",
    "    stop_randomization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2dca9ad-448c-499f-9e87-8d4069d3f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 1024\n",
    "train_dataloader = training_dataset.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=8\n",
    ")\n",
    "val_dataloader = validation_dataset.to_dataloader(\n",
    "    train=False, \n",
    "    batch_size=batch_size * 2, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd47e47f-e494-4e4a-afcd-4b076e41b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFT 모델 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# TFT 모델 생성\n",
    "print(\"TFT 모델 생성 중...\")\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training_dataset,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=64,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=32,\n",
    "    loss=SMAPE(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# 학습 설정\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filename=\"tft-{epoch:02d}-{val_loss:.4f}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    gradient_clip_val=0.5,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d043bc-8476-4eb8-b092-730eb405f5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 2.1 K \n",
      "3  | prescalers                         | ModuleDict                      | 1.0 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 20.9 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 95.8 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 88.1 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "413 K     Trainable params\n",
      "0         Non-trainable params\n",
      "413 K     Total params\n",
      "1.653     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761eae59bc97415fa126b77f4dab7127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "print(\"모델 학습 시작...\")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890eadb6-35a1-4851-b020-a1da4007818c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/workspace/Electricity-Load-Forecasting'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 최적 모델 로드\u001b[39;00m\n\u001b[1;32m      2\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m checkpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path\n\u001b[0;32m----> 3\u001b[0m best_tft \u001b[38;5;241m=\u001b[39m \u001b[43mTemporalFusionTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:139\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:160\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     map_location \u001b[38;5;241m=\u001b[39m cast(_MAP_LOCATION_TYPE, \u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m--> 160\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m    163\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m    164\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/cloud_io.py:50\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[1;32m     47\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/spec.py:1154\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1154\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py:183\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py:287\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py:292\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 292\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    294\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/workspace/Electricity-Load-Forecasting'"
     ]
    }
   ],
   "source": [
    "# 최적 모델 로드\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779ffad-700f-4e06-9e51-603d0c455894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측을 위한 전체 데이터셋 준비\n",
    "print(\"예측 준비 중...\")\n",
    "\n",
    "# test 데이터에 대한 전처리\n",
    "test_df = create_time_features(test_df)\n",
    "\n",
    "# train과 test 결합하여 전체 시계열 데이터 생성\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "all_data = all_data.sort_values(['건물번호', '일시']).reset_index(drop=True)\n",
    "all_data['time_idx'] = all_data.groupby('건물번호').cumcount()\n",
    "\n",
    "# test 기간 동안의 예측 수행\n",
    "predictions = []\n",
    "\n",
    "for building_num in all_data['건물번호'].unique():\n",
    "    building_data = all_data[all_data['건물번호'] == building_num].copy()\n",
    "    \n",
    "    # 예측 시작 시점 찾기\n",
    "    test_start_idx = building_data[building_data['일시'] >= test_df['일시'].min()].index[0]\n",
    "    \n",
    "    # 예측 수행\n",
    "    for i in range(0, len(test_df[test_df['건물번호'] == building_num]), max_prediction_length):\n",
    "        # 현재까지의 데이터로 dataset 생성\n",
    "        current_data = building_data.iloc[:test_start_idx + i].copy()\n",
    "        \n",
    "        if len(current_data) < max_encoder_length:\n",
    "            continue\n",
    "            \n",
    "        # 예측용 dataset 생성\n",
    "        prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
    "            training_dataset,\n",
    "            current_data,\n",
    "            predict=True,\n",
    "            stop_randomization=True\n",
    "        )\n",
    "        \n",
    "        # 예측\n",
    "        prediction_dataloader = prediction_dataset.to_dataloader(\n",
    "            train=False, \n",
    "            batch_size=1, \n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        raw_predictions = best_tft.predict(\n",
    "            prediction_dataloader,\n",
    "            mode=\"raw\",\n",
    "            return_x=True\n",
    "        )\n",
    "        \n",
    "        # 예측값 추출\n",
    "        for j, pred in enumerate(raw_predictions.output):\n",
    "            if j < len(test_df[test_df['건물번호'] == building_num]) - i:\n",
    "                predictions.append({\n",
    "                    '건물번호': building_num,\n",
    "                    '일시': test_df[(test_df['건물번호'] == building_num)].iloc[i + j]['일시'],\n",
    "                    'prediction': pred[0].item()\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d0ef8-d5ba-4510-a44a-3fbca735660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 정리\n",
    "print(\"예측 결과 정리 중...\")\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "# 로그 역변환\n",
    "predictions_df['전력소비량(kWh)'] = np.expm1(predictions_df['prediction'])\n",
    "\n",
    "# submission 형식에 맞게 정리\n",
    "final_predictions = predictions_df[['건물번호', '일시', '전력소비량(kWh)']]\n",
    "final_predictions['일시'] = final_predictions['일시'].dt.strftime('%Y-%m-%d %H')\n",
    "\n",
    "# sample_submission과 동일한 순서로 정렬\n",
    "submission_df = submission_df.merge(\n",
    "    final_predictions, \n",
    "    on=['건물번호', '일시'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 누락된 값 처리 (건물별 평균값으로 대체)\n",
    "for building in submission_df['건물번호'].unique():\n",
    "    building_mask = submission_df['건물번호'] == building\n",
    "    if submission_df.loc[building_mask, '전력소비량(kWh)'].isna().any():\n",
    "        mean_value = train_df[train_df['건물번호'] == building]['전력소비량(kWh)'].mean()\n",
    "        submission_df.loc[building_mask, '전력소비량(kWh)'] = submission_df.loc[building_mask, '전력소비량(kWh)'].fillna(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc713e9a-ccd4-44ad-b057-872a1804e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 저장\n",
    "submission_df[['num_date_time', '건물번호', '일시', '전력소비량(kWh)']].to_csv('submission_tft.csv', index=False)\n",
    "print(\"제출 파일 저장 완료: submission_tft.csv\")\n",
    "\n",
    "# 예측 결과 시각화\n",
    "plt.figure(figsize=(15, 8))\n",
    "sample_building = submission_df['건물번호'].unique()[0]\n",
    "sample_data = submission_df[submission_df['건물번호'] == sample_building]\n",
    "\n",
    "plt.plot(range(len(sample_data)), sample_data['전력소비량(kWh)'], label=f'Building {sample_building} Predictions')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('전력소비량(kWh)')\n",
    "plt.title('TFT Model Predictions Sample')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tft_predictions_sample.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"모델 학습 및 예측 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52be810-866e-4fbd-b6a5-c63f789cc055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
