{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 1: 함수 선언 및 import\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import sklearn\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 시계열 교차검증을 위한 sklearn 함수들\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import random as rn\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "RANDOM_SEED = 2024\n",
    "np.random.seed(RANDOM_SEED)\n",
    "rn.seed(RANDOM_SEED)\n",
    "\n",
    "def smape(gt, preds):\n",
    "    \"\"\"SMAPE 계산 함수\"\"\"\n",
    "    gt = np.array(gt)\n",
    "    preds = np.array(preds)\n",
    "    v = 2 * abs(preds - gt) / (abs(preds) + abs(gt) + 1e-9)  # 0 division 방지\n",
    "    score = np.mean(v) * 100\n",
    "    return score\n",
    "\n",
    "def weighted_mse(alpha=1):\n",
    "    \"\"\"XGBoost용 가중치 MSE 손실함수\"\"\"\n",
    "    def weighted_mse_fixed(label, pred):\n",
    "        residual = (label - pred).astype(\"float\")\n",
    "        grad = np.where(residual > 0, -2 * alpha * residual, -2 * residual)\n",
    "        hess = np.where(residual > 0, 2 * alpha, 2.0)\n",
    "        return grad, hess\n",
    "    return weighted_mse_fixed\n",
    "\n",
    "def custom_smape(preds, dtrain):\n",
    "    \"\"\"XGBoost용 사용자 정의 SMAPE 평가함수\"\"\"\n",
    "    labels = dtrain.get_label()\n",
    "    return 'custom_smape', np.mean(2 * abs(preds - labels) / (abs(preds) + abs(labels) + 1e-9)) * 100\n",
    "\n",
    "def pseudo_huber_loss(delta=1.0):\n",
    "    \"\"\"로버스트 Pseudo-Huber 손실함수 (선택사항)\"\"\"\n",
    "    def pseudo_huber_objective(label, pred):\n",
    "        residual = label - pred\n",
    "        scale = delta\n",
    "        grad = residual / np.sqrt(1 + (residual / scale) ** 2)\n",
    "        hess = 1 / (1 + (residual / scale) ** 2) ** (3/2)\n",
    "        return grad, hess\n",
    "    return pseudo_huber_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 2: 데이터 불러오기 및 기본 전처리\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "building_info = pd.read_csv('building_info.csv')\n",
    "\n",
    "# 컬럼명 영문 변환\n",
    "train = train.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(°C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "})\n",
    "if 'num_date_time' in train.columns:\n",
    "    train.drop('num_date_time', axis=1, inplace=True)\n",
    "\n",
    "test = test.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(°C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "})\n",
    "if 'num_date_time' in test.columns:\n",
    "    test.drop('num_date_time', axis=1, inplace=True)\n",
    "\n",
    "building_info = building_info.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '건물유형': 'building_type',\n",
    "    '연면적(m2)': 'total_area',\n",
    "    '냉방면적(m2)': 'cooling_area',\n",
    "    '태양광용량(kW)': 'solar_power_capacity',\n",
    "    'ESS저장용량(kWh)': 'ess_capacity',\n",
    "    'PCS용량(kW)': 'pcs_capacity'\n",
    "})\n",
    "\n",
    "# 건물유형 영문 번역\n",
    "translation_dict = {\n",
    "    '건물기타': 'Other Buildings',\n",
    "    '공공': 'Public',\n",
    "    '대학교': 'University',\n",
    "    '데이터센터': 'Data Center',\n",
    "    '백화점및아울렛': 'Department Store and Outlet',\n",
    "    '병원': 'Hospital',\n",
    "    '상용': 'Commercial',\n",
    "    '아파트': 'Apartment',\n",
    "    '연구소': 'Research Institute',\n",
    "    '지식산업센터': 'Knowledge Industry Center',\n",
    "    '할인마트': 'Discount Mart',\n",
    "    '호텔및리조트': 'Hotel and Resort'\n",
    "}\n",
    "\n",
    "building_info['building_type'] = building_info['building_type'].replace(translation_dict)\n",
    "\n",
    "# 태양광/ESS 활용 여부 플래그 생성\n",
    "building_info['solar_power_utility'] = np.where(building_info.solar_power_capacity != '-', 1, 0)\n",
    "building_info['ess_utility'] = np.where(building_info.ess_capacity != '-', 1, 0)\n",
    "\n",
    "# 건물 정보 병합\n",
    "train = pd.merge(train, building_info, on='building_number', how='left')\n",
    "test = pd.merge(test, building_info, on='building_number', how='left')\n",
    "\n",
    "# 냉방면적비 추가\n",
    "train[\"cooling_ratio\"] = (\n",
    "    train[\"cooling_area\"] / train[\"total_area\"]\n",
    ").replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "test[\"cooling_ratio\"] = (\n",
    "    test[\"cooling_area\"] / test[\"total_area\"]\n",
    ").replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# 결측치 보간 (선형보간 사용)\n",
    "train['windspeed'] = train['windspeed'].interpolate(method='linear')\n",
    "train['humidity'] = train['humidity'].interpolate(method='linear')\n",
    "test['windspeed'] = test['windspeed'].interpolate(method='linear')\n",
    "test['humidity'] = test['humidity'].interpolate(method='linear')\n",
    "\n",
    "print(\"데이터 기본 전처리 완료\")\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
    "print(f\"Building types: {train.building_type.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8372425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 3: 날짜/시간 피처 생성 (포리에 변환 포함)\n",
    "train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
    "test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "# 기본 시간 피처\n",
    "for df in [train, test]:\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day_of_week'] = df['date_time'].dt.dayofweek  # 0=월요일, 6=일요일\n",
    "    df['day_of_year'] = df['date_time'].dt.dayofyear\n",
    "    df['week_of_year'] = df['date_time'].dt.isocalendar().week\n",
    "\n",
    "# *** 개선사항 1: 포리에 변환 (다중 계절성) ***\n",
    "for df in [train, test]:\n",
    "    # 시간별 계절성 (24시간 주기)\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    \n",
    "    # 일별 계절성 (7일 주기)\n",
    "    df['sin_day_of_week'] = np.sin(2 * np.pi * df['day_of_week'] / 7.0)\n",
    "    df['cos_day_of_week'] = np.cos(2 * np.pi * df['day_of_week'] / 7.0)\n",
    "    \n",
    "    # 월별 계절성 (12개월 주기)\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12.0)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12.0)\n",
    "    \n",
    "    # 연간 계절성 (365일 주기)\n",
    "    df['sin_day_of_year'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "    df['cos_day_of_year'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "\n",
    "# *** 개선사항 2: 휴일 및 특수일 플래그 ***\n",
    "# 한국 공휴일\n",
    "korean_holidays = [\n",
    "    '2024-06-06',  # 현충일 (목)\n",
    "    '2024-08-15'   # 광복절 (목)\n",
    "]\n",
    "\n",
    "for df in [train, test]:\n",
    "    # 기본 휴일 (주말 + 공휴일)\n",
    "    df['is_holiday'] = np.where(\n",
    "        (df['day_of_week'] >= 5) | \n",
    "        (df['date_time'].dt.strftime('%Y-%m-%d').isin(korean_holidays)), \n",
    "        1, 0\n",
    "    )\n",
    "    \n",
    "    # 주말 vs 평일\n",
    "    df['is_weekend'] = np.where(df['day_of_week'] >= 5, 1, 0)\n",
    "    \n",
    "    # 시간대별 구분\n",
    "    df['time_period'] = pd.cut(df['hour'], \n",
    "                              bins=[-1, 6, 12, 18, 24], \n",
    "                              labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "\n",
    "# 대형마트 의무휴업 일요일\n",
    "mart_closure_sundays = [\n",
    "    '2024-06-09', '2024-06-23',  # 6월\n",
    "    '2024-07-14', '2024-07-28',  # 7월\n",
    "    '2024-08-11', '2024-08-25'   # 8월\n",
    "]\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['mart_closure_sunday'] = np.where(\n",
    "        (df['day_of_week'] == 6) & \n",
    "        (df['date_time'].dt.strftime('%Y-%m-%d').isin(mart_closure_sundays)), \n",
    "        1, 0\n",
    "    )\n",
    "\n",
    "print(\"시간 피처 및 포리에 변환 완료\")\n",
    "print(\"생성된 시간 피처들:\", [col for col in train.columns if any(x in col for x in ['sin_', 'cos_', 'is_', 'time_'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 4: 날씨 파생 피처 및 냉난방도시간(CDH/HDH) 생성\n",
    "\n",
    "def calculate_day_values(dataframe, target_column, output_column, aggregation_func):\n",
    "    \"\"\"일별 통계값 계산 함수\"\"\"\n",
    "    result_dict = {}\n",
    "    grouped = dataframe.groupby(['building_number', 'month', 'day'])[target_column].agg(aggregation_func)\n",
    "    \n",
    "    for (building, month, day), value in grouped.items():\n",
    "        result_dict.setdefault(building, {}).setdefault(month, {})[day] = value\n",
    "    \n",
    "    dataframe[output_column] = [\n",
    "        result_dict.get(row['building_number'], {}).get(row['month'], {}).get(row['day'], None)\n",
    "        for _, row in dataframe.iterrows()\n",
    "    ]\n",
    "\n",
    "# *** 개선사항 3: 날씨 변수 파생 피처 강화 ***\n",
    "for df in [train, test]:\n",
    "    # 일별 온도 통계\n",
    "    calculate_day_values(df, 'temperature', 'day_max_temperature', 'max')\n",
    "    calculate_day_values(df, 'temperature', 'day_mean_temperature', 'mean')\n",
    "    calculate_day_values(df, 'temperature', 'day_min_temperature', 'min')\n",
    "    \n",
    "    # 일교차\n",
    "    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']\n",
    "    \n",
    "    # THI (불쾌지수) - 더 정확한 공식\n",
    "    df['THI'] = 9/5 * df['temperature'] - 0.55 * (1 - df['humidity']/100) * (9/5 * df['temperature'] - 26) + 32\n",
    "    \n",
    "    # WCT (체감온도) - 풍속 고려\n",
    "    df['WCT'] = 13.12 + 0.6125 * df['temperature'] - 11.37 * (df['windspeed']**0.16) + \\\n",
    "                0.3965 * (df['windspeed']**0.16) * df['temperature']\n",
    "    \n",
    "    # 습구온도 근사치\n",
    "    df['wet_bulb_temp'] = df['temperature'] * np.arctan(0.151977 * np.sqrt(df['humidity'] + 8.313659)) + \\\n",
    "                         np.arctan(df['temperature'] + df['humidity']) - \\\n",
    "                         np.arctan(df['humidity'] - 1.676331) + \\\n",
    "                         0.00391838 * (df['humidity']**1.5) * np.arctan(0.023101 * df['humidity']) - 4.686035\n",
    "    \n",
    "    # 절대습도 계산\n",
    "    df['absolute_humidity'] = (6.112 * np.exp((17.67 * df['temperature'])/(df['temperature'] + 243.5)) * \n",
    "                              df['humidity'] * 2.1674) / (273.15 + df['temperature'])\n",
    "\n",
    "# *** 개선사항 4: CDH/HDH (냉난방도시간) 개선 ***\n",
    "def enhanced_CDH_HDH(temperature_series, cooling_base=26, heating_base=18):\n",
    "    \"\"\"개선된 냉난방도시간 계산 (11시간 슬라이딩 윈도우)\"\"\"\n",
    "    temp = np.array(temperature_series)\n",
    "    \n",
    "    # CDH (냉방도시간)\n",
    "    cdh_values = np.maximum(0, temp - cooling_base)\n",
    "    cdh_cumsum = np.cumsum(cdh_values)\n",
    "    cdh_result = np.concatenate([\n",
    "        cdh_cumsum[:11], \n",
    "        cdh_cumsum[11:] - cdh_cumsum[:-11]\n",
    "    ])\n",
    "    \n",
    "    # HDH (난방도시간) \n",
    "    hdh_values = np.maximum(0, heating_base - temp)\n",
    "    hdh_cumsum = np.cumsum(hdh_values)\n",
    "    hdh_result = np.concatenate([\n",
    "        hdh_cumsum[:11],\n",
    "        hdh_cumsum[11:] - hdh_cumsum[:-11]\n",
    "    ])\n",
    "    \n",
    "    return cdh_result, hdh_result\n",
    "\n",
    "# 건물별 CDH/HDH 계산\n",
    "def calculate_and_add_cdh_hdh(dataframe):\n",
    "    cdhs, hdhs = [], []\n",
    "    for building_num in range(1, 101):\n",
    "        building_data = dataframe[dataframe['building_number'] == building_num]\n",
    "        if len(building_data) > 0:\n",
    "            temp_values = building_data['temperature'].values\n",
    "            cdh, hdh = enhanced_CDH_HDH(temp_values)\n",
    "            cdhs.extend(cdh)\n",
    "            hdhs.extend(hdh)\n",
    "    return cdhs, hdhs\n",
    "\n",
    "# CDH/HDH 계산 및 추가\n",
    "train_cdh, train_hdh = calculate_and_add_cdh_hdh(train)\n",
    "train['CDH'] = train_cdh\n",
    "train['HDH'] = train_hdh\n",
    "\n",
    "test_cdh, test_hdh = calculate_and_add_cdh_hdh(test)\n",
    "test['CDH'] = test_cdh  \n",
    "test['HDH'] = test_hdh\n",
    "\n",
    "print(\"날씨 파생 피처 및 CDH/HDH 계산 완료\")\n",
    "print(\"추가된 날씨 피처:\", ['day_temperature_range', 'THI', 'WCT', 'wet_bulb_temp', 'absolute_humidity', 'CDH', 'HDH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b27e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 5: 과거 부하 래그 피처 생성 (핵심 개선사항)\n",
    "\n",
    "def create_lag_features(df, target_col='power_consumption', building_col='building_number'):\n",
    "    \"\"\"\n",
    "    시계열 래그 피처 생성\n",
    "    - 1시간, 24시간(일주기), 168시간(주주기) 래그\n",
    "    - 롤링 평균 및 표준편차\n",
    "    \"\"\"\n",
    "    df_with_lags = df.copy()\n",
    "    df_with_lags = df_with_lags.sort_values([building_col, 'date_time'])\n",
    "    \n",
    "    # 건물별로 래그 피처 생성\n",
    "    lag_features = []\n",
    "    for building_num in df[building_col].unique():\n",
    "        building_data = df_with_lags[df_with_lags[building_col] == building_num].copy()\n",
    "        \n",
    "        if len(building_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        building_data = building_data.sort_values('date_time')\n",
    "        \n",
    "        # *** 개선사항 5: 핵심 래그 피처들 ***\n",
    "        # 1시간 전 (직전 시간)\n",
    "        building_data['lag_1h'] = building_data[target_col].shift(1)\n",
    "        \n",
    "        # 24시간 전 (전날 동시간)  \n",
    "        building_data['lag_24h'] = building_data[target_col].shift(24)\n",
    "        \n",
    "        # 168시간 전 (전주 동시간동요일)\n",
    "        building_data['lag_168h'] = building_data[target_col].shift(168)\n",
    "        \n",
    "        # 롤링 평균 (최근 3시간, 24시간, 168시간)\n",
    "        building_data['rolling_mean_3h'] = building_data[target_col].rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        building_data['rolling_mean_24h'] = building_data[target_col].rolling(window=24, min_periods=1).mean().shift(1)\n",
    "        building_data['rolling_mean_168h'] = building_data[target_col].rolling(window=168, min_periods=1).mean().shift(1)\n",
    "        \n",
    "        # 롤링 표준편차\n",
    "        building_data['rolling_std_24h'] = building_data[target_col].rolling(window=24, min_periods=1).std().shift(1)\n",
    "        building_data['rolling_std_168h'] = building_data[target_col].rolling(window=168, min_periods=1).std().shift(1)\n",
    "        \n",
    "        # 최대/최소값 (최근 24시간)\n",
    "        building_data['rolling_max_24h'] = building_data[target_col].rolling(window=24, min_periods=1).max().shift(1)\n",
    "        building_data['rolling_min_24h'] = building_data[target_col].rolling(window=24, min_periods=1).min().shift(1)\n",
    "        \n",
    "        # 변화율 피처\n",
    "        building_data['pct_change_1h'] = building_data[target_col].pct_change(periods=1)\n",
    "        building_data['pct_change_24h'] = building_data[target_col].pct_change(periods=24)\n",
    "        \n",
    "        # 전날 동시간 대비 차이\n",
    "        building_data['diff_24h'] = building_data[target_col] - building_data['lag_24h']\n",
    "        \n",
    "        lag_features.append(building_data)\n",
    "    \n",
    "    return pd.concat(lag_features, ignore_index=True)\n",
    "\n",
    "# 훈련 데이터에 래그 피처 추가 (target이 있는 경우에만)\n",
    "print(\"과거 부하 래그 피처 생성 중...\")\n",
    "train_with_lags = create_lag_features(train)\n",
    "\n",
    "# 테스트 데이터의 경우, 훈련 데이터의 마지막 값들을 이용하여 초기 래그값 설정\n",
    "def create_test_lag_features(train_df, test_df, target_col='power_consumption', building_col='building_number'):\n",
    "    \"\"\"테스트 데이터용 래그 피처 생성\"\"\"\n",
    "    test_with_lags = test_df.copy()\n",
    "    \n",
    "    # 각 건물별로 처리\n",
    "    for building_num in test_df[building_col].unique():\n",
    "        # 해당 건물의 훈련 데이터 마지막 부분\n",
    "        train_building = train_df[train_df[building_col] == building_num].copy()\n",
    "        test_building = test_df[test_df[building_col] == building_num].copy()\n",
    "        \n",
    "        if len(train_building) == 0 or len(test_building) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 시간순 정렬\n",
    "        train_building = train_building.sort_values('date_time')\n",
    "        test_building = test_building.sort_values('date_time')\n",
    "        \n",
    "        # 연결된 시계열 생성 (훈련 데이터 끝 + 테스트 데이터)\n",
    "        # 테스트 데이터에는 power_consumption이 없으므로 NaN으로 설정\n",
    "        test_building_temp = test_building.copy()\n",
    "        test_building_temp[target_col] = np.nan\n",
    "        \n",
    "        # 훈련 데이터의 마지막 168개 시점 + 테스트 데이터\n",
    "        recent_train = train_building.tail(168)\n",
    "        combined = pd.concat([recent_train, test_building_temp], ignore_index=True)\n",
    "        combined = combined.sort_values('date_time')\n",
    "        \n",
    "        # 래그 피처들을 계산 (NaN인 target에 대해서는 이전 값들로만 계산)\n",
    "        for i in range(len(recent_train), len(combined)):\n",
    "            # 1시간 전\n",
    "            if i >= 1:\n",
    "                combined.iloc[i, combined.columns.get_loc('lag_1h')] = combined.iloc[i-1][target_col] if not pd.isna(combined.iloc[i-1][target_col]) else recent_train.iloc[-1][target_col]\n",
    "            \n",
    "            # 24시간 전  \n",
    "            if i >= 24:\n",
    "                combined.iloc[i, combined.columns.get_loc('lag_24h')] = combined.iloc[i-24][target_col]\n",
    "                \n",
    "            # 168시간 전\n",
    "            if i >= 168:\n",
    "                combined.iloc[i, combined.columns.get_loc('lag_168h')] = combined.iloc[i-168][target_col]\n",
    "        \n",
    "        # 테스트 부분만 추출하여 원본에 업데이트\n",
    "        test_part = combined.iloc[len(recent_train):].copy()\n",
    "        test_with_lags.loc[test_with_lags[building_col] == building_num, \n",
    "                          [col for col in test_part.columns if 'lag_' in col or 'rolling_' in col]] = \\\n",
    "                          test_part[[col for col in test_part.columns if 'lag_' in col or 'rolling_' in col]].values\n",
    "    \n",
    "    return test_with_lags\n",
    "\n",
    "\n",
    "# 실제 래그 피처 생성은 타겟이 없는 테스트 데이터 특성상 간단하게 처리\n",
    "# 여기서는 기본 통계값으로 대체\n",
    "for df_name, df in [('train', train_with_lags), ('test', test)]:\n",
    "    if df_name == 'test':\n",
    "        # 테스트 데이터는 래그 피처를 0 또는 건물별 평균으로 초기화\n",
    "        lag_cols = ['lag_1h', 'lag_24h', 'lag_168h', 'rolling_mean_3h', 'rolling_mean_24h', \n",
    "                   'rolling_mean_168h', 'rolling_std_24h', 'rolling_std_168h', \n",
    "                   'rolling_max_24h', 'rolling_min_24h', 'pct_change_1h', 'pct_change_24h', 'diff_24h']\n",
    "        \n",
    "        for col in lag_cols:\n",
    "            df[col] = 0  # 또는 건물별 평균값으로 대체 가능\n",
    "\n",
    "# train 데이터를 업데이트\n",
    "train = train_with_lags\n",
    "\n",
    "print(\"래그 피처 생성 완료\")\n",
    "print(f\"생성된 래그 피처: {[col for col in train.columns if any(x in col for x in ['lag_', 'rolling_', 'pct_change_', 'diff_'])]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005325b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 6: 개선된 이상치 탐지 및 처리 (가중치 기반)\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def detect_outliers_with_weights(df, target_col='power_consumption', building_col='building_number'):\n",
    "    \"\"\"\n",
    "    이상치를 완전 제거하지 않고 가중치를 부여하는 방식\n",
    "    - 센서 오류/정전: 제거\n",
    "    - 이벤트성 이상치: 가중치 감소\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    df_result['sample_weight'] = 1.0  # 기본 가중치\n",
    "    df_result['is_extreme'] = 0       # 이벤트성 이상치 플래그\n",
    "    \n",
    "    outlier_indices_to_remove = []    # 완전 제거할 인덱스\n",
    "    \n",
    "    # 건물 유형별 완화 처리 대상\n",
    "    RELAX_TYPES = [\"Public\", \"Other Buildings\", \"Hotel and Resort\", \"Department Store and Outlet\"]\n",
    "    \n",
    "    for building_num in df[building_col].unique():\n",
    "        building_mask = df_result[building_col] == building_num\n",
    "        building_data = df_result[building_mask].copy()\n",
    "        building_type = building_data['building_type'].iloc[0] if len(building_data) > 0 else \"Unknown\"\n",
    "        \n",
    "        if len(building_data) < 24:  # 데이터가 너무 적으면 스킵\n",
    "            continue\n",
    "            \n",
    "        power_values = building_data[target_col].astype(float)\n",
    "        \n",
    "        # 1. 센서 오류/정전 탐지 (완전 제거 대상)\n",
    "        sensor_errors = []\n",
    "        \n",
    "        # 1-1. 연속된 0값 또는 음수값\n",
    "        zero_negative_mask = (power_values <= 0.01)\n",
    "        consecutive_zeros = []\n",
    "        count = 0\n",
    "        for i, is_zero in enumerate(zero_negative_mask):\n",
    "            if is_zero:\n",
    "                count += 1\n",
    "            else:\n",
    "                if count >= 4:  # 4시간 이상 연속 0\n",
    "                    consecutive_zeros.extend(range(i-count, i))\n",
    "                count = 0\n",
    "        sensor_errors.extend([building_data.index[i] for i in consecutive_zeros])\n",
    "        \n",
    "        # 1-2. 급격한 하락 (전시간 대비 95% 이상 감소)\n",
    "        prev_values = power_values.shift(1)\n",
    "        sudden_drop_mask = ((prev_values > 0) & \n",
    "                           ((prev_values - power_values) / (prev_values + 1e-9) >= 0.95))\n",
    "        sensor_errors.extend(building_data.index[sudden_drop_mask])\n",
    "        \n",
    "        # 1-3. 황당하게 큰 값 (99.9% 분위수의 5배 초과)\n",
    "        upper_bound = np.percentile(power_values.dropna(), 99.9) * 5\n",
    "        extreme_high_mask = power_values > upper_bound\n",
    "        sensor_errors.extend(building_data.index[extreme_high_mask])\n",
    "        \n",
    "        # 2. 이벤트성 이상치 탐지 (가중치 감소 대상)\n",
    "        # IQR 방법으로 이상치 탐지\n",
    "        Q1 = power_values.quantile(0.25)\n",
    "        Q3 = power_values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # 건물 유형별 다른 임계값 적용\n",
    "        if building_type in RELAX_TYPES:\n",
    "            outlier_factor = 3.0  # 더 관대한 기준\n",
    "        else:\n",
    "            outlier_factor = 2.0  # 일반적인 기준\n",
    "            \n",
    "        lower_bound = Q1 - outlier_factor * IQR\n",
    "        upper_bound = Q3 + outlier_factor * IQR\n",
    "        \n",
    "        moderate_outliers = building_data.index[\n",
    "            (power_values < lower_bound) | (power_values > upper_bound)\n",
    "        ]\n",
    "        \n",
    "        # 센서 오류와 겹치지 않는 순수 이벤트성 이상치만 선별\n",
    "        event_outliers = [idx for idx in moderate_outliers if idx not in sensor_errors]\n",
    "        \n",
    "        # 3. 결과 적용\n",
    "        # 센서 오류는 완전 제거 목록에 추가\n",
    "        outlier_indices_to_remove.extend(sensor_errors)\n",
    "        \n",
    "        # 이벤트성 이상치는 가중치만 감소\n",
    "        for idx in event_outliers:\n",
    "            if building_type in RELAX_TYPES:\n",
    "                df_result.loc[idx, 'sample_weight'] = 0.3  # 관대한 처리\n",
    "                df_result.loc[idx, 'is_extreme'] = 1\n",
    "            else:\n",
    "                df_result.loc[idx, 'sample_weight'] = 0.1  # 강한 페널티\n",
    "    \n",
    "    return df_result, list(set(outlier_indices_to_remove))\n",
    "\n",
    "# 이상치 탐지 및 가중치 부여\n",
    "print(\"이상치 탐지 및 가중치 부여 중...\")\n",
    "train_weighted, remove_indices = detect_outliers_with_weights(train)\n",
    "\n",
    "print(f\"센서 오류/정전으로 제거할 샘플: {len(remove_indices)}개\")\n",
    "print(f\"가중치 감소 대상 (이벤트성): {sum(train_weighted['sample_weight'] < 1.0)}개\")\n",
    "print(f\"극한 이벤트 플래그: {sum(train_weighted['is_extreme'] == 1)}개\")\n",
    "\n",
    "# 센서 오류만 제거하고 나머지는 가중치로 처리\n",
    "train_cleaned = train_weighted.drop(index=remove_indices).reset_index(drop=True)\n",
    "\n",
    "# 테스트 데이터에는 기본값 설정\n",
    "test['sample_weight'] = 1.0\n",
    "test['is_extreme'] = 0\n",
    "\n",
    "print(f\"최종 훈련 데이터 크기: {train_cleaned.shape}\")\n",
    "print(\"이상치 처리 완료 (가중치 기반)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 7: 전력 소비 통계 피처 생성 (과거 데이터 기반)\n",
    "\n",
    "def create_power_statistics_features(df, target_col='power_consumption', building_col='building_number'):\n",
    "    \"\"\"전력 소비 패턴 기반 통계 피처 생성\"\"\"\n",
    "    \n",
    "    # 건물별, 시간대별, 요일별 평균/표준편차\n",
    "    power_hour_dayofweek_stats = df.groupby([building_col, 'hour', 'day_of_week'])[target_col].agg(['mean', 'std']).reset_index()\n",
    "    power_hour_dayofweek_stats.columns = [building_col, 'hour', 'day_of_week', 'power_hour_dayofweek_mean', 'power_hour_dayofweek_std']\n",
    "    \n",
    "    # 건물별, 시간대별 평균/표준편차  \n",
    "    power_hour_stats = df.groupby([building_col, 'hour'])[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    power_hour_stats.columns = [building_col, 'hour', 'power_hour_mean', 'power_hour_std', 'power_hour_min', 'power_hour_max']\n",
    "    \n",
    "    # 건물별, 요일별 평균/표준편차\n",
    "    power_dayofweek_stats = df.groupby([building_col, 'day_of_week'])[target_col].agg(['mean', 'std']).reset_index()\n",
    "    power_dayofweek_stats.columns = [building_col, 'day_of_week', 'power_dayofweek_mean', 'power_dayofweek_std']\n",
    "    \n",
    "    # 건물별, 월별 평균/표준편차\n",
    "    power_month_stats = df.groupby([building_col, 'month'])[target_col].agg(['mean', 'std']).reset_index()  \n",
    "    power_month_stats.columns = [building_col, 'month', 'power_month_mean', 'power_month_std']\n",
    "    \n",
    "    # 건물별 전체 통계\n",
    "    power_building_stats = df.groupby(building_col)[target_col].agg(['mean', 'std', 'min', 'max', 'median']).reset_index()\n",
    "    power_building_stats.columns = [building_col, 'power_building_mean', 'power_building_std', 'power_building_min', 'power_building_max', 'power_building_median']\n",
    "    \n",
    "    return (power_hour_dayofweek_stats, power_hour_stats, power_dayofweek_stats, \n",
    "            power_month_stats, power_building_stats)\n",
    "\n",
    "# 통계 피처 계산\n",
    "print(\"전력 소비 통계 피처 생성 중...\")\n",
    "stats_features = create_power_statistics_features(train_cleaned)\n",
    "\n",
    "power_hour_dayofweek_stats, power_hour_stats, power_dayofweek_stats, power_month_stats, power_building_stats = stats_features\n",
    "\n",
    "# 훈련 데이터에 통계 피처 병합\n",
    "train_final = train_cleaned.copy()\n",
    "\n",
    "train_final = train_final.merge(power_hour_dayofweek_stats, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "train_final = train_final.merge(power_hour_stats, on=['building_number', 'hour'], how='left')\n",
    "train_final = train_final.merge(power_dayofweek_stats, on=['building_number', 'day_of_week'], how='left')  \n",
    "train_final = train_final.merge(power_month_stats, on=['building_number', 'month'], how='left')\n",
    "train_final = train_final.merge(power_building_stats, on='building_number', how='left')\n",
    "\n",
    "# 테스트 데이터에도 동일한 통계 피처 병합\n",
    "test_final = test.copy()\n",
    "\n",
    "test_final = test_final.merge(power_hour_dayofweek_stats, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "test_final = test_final.merge(power_hour_stats, on=['building_number', 'hour'], how='left')\n",
    "test_final = test_final.merge(power_dayofweek_stats, on=['building_number', 'day_of_week'], how='left')\n",
    "test_final = test_final.merge(power_month_stats, on=['building_number', 'month'], how='left')\n",
    "test_final = test_final.merge(power_building_stats, on='building_number', how='left')\n",
    "\n",
    "# 결측치 처리 (통계 피처의 경우)\n",
    "stat_columns = [col for col in train_final.columns if 'power_' in col and any(x in col for x in ['_mean', '_std', '_min', '_max', '_median'])]\n",
    "\n",
    "for col in stat_columns:\n",
    "    train_final[col] = train_final[col].fillna(train_final[col].median())\n",
    "    test_final[col] = test_final[col].fillna(train_final[col].median())\n",
    "\n",
    "# 파생 통계 피처 추가\n",
    "for df in [train_final, test_final]:\n",
    "    # 현재 시간대 평균 대비 비율 (train에서만 계산 가능)\n",
    "    if 'power_consumption' in df.columns:\n",
    "        df['power_vs_hour_mean_ratio'] = df['power_consumption'] / (df['power_hour_mean'] + 1e-9)\n",
    "        df['power_vs_building_mean_ratio'] = df['power_consumption'] / (df['power_building_mean'] + 1e-9)\n",
    "    \n",
    "    # 시간대별 변동성 지표\n",
    "    df['power_hour_cv'] = df['power_hour_std'] / (df['power_hour_mean'] + 1e-9)  # 변동계수\n",
    "    df['power_hour_range'] = df['power_hour_max'] - df['power_hour_min']\n",
    "\n",
    "print(\"전력 소비 통계 피처 생성 완료\")\n",
    "print(f\"생성된 통계 피처 수: {len(stat_columns)}\")\n",
    "print(\"추가 파생 통계 피처:\", ['power_vs_hour_mean_ratio', 'power_vs_building_mean_ratio', 'power_hour_cv', 'power_hour_range'])\n",
    "\n",
    "# 데이터 업데이트\n",
    "train = train_final\n",
    "test = test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 8: 시계열 교차검증 및 개선된 모델링\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 모델링용 피처 선택\n",
    "def prepare_model_features(train_df, test_df):\n",
    "    \"\"\"모델링용 피처 준비\"\"\"\n",
    "    \n",
    "    # 제거할 컬럼들\n",
    "    drop_columns = [\n",
    "        'solar_power_capacity', 'ess_capacity', 'pcs_capacity',\n",
    "        'power_consumption', 'rainfall', 'sunshine', 'solar_radiation',\n",
    "        'hour', 'day', 'month', 'day_of_week', 'date_time', 'day_of_year', 'week_of_year'\n",
    "    ]\n",
    "    \n",
    "    # 존재하는 컬럼만 제거\n",
    "    drop_columns = [col for col in drop_columns if col in train_df.columns]\n",
    "    \n",
    "    X = train_df.drop(columns=drop_columns)\n",
    "    y = train_df['power_consumption'].astype(float)\n",
    "    X_test = test_df.drop(columns=[col for col in drop_columns if col in test_df.columns and col != 'power_consumption'])\n",
    "    \n",
    "    # 범주형 변수 처리 (원핫 인코딩)\n",
    "    categorical_columns = ['building_type', 'time_period']\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in X.columns:\n",
    "            # 훈련 데이터 원핫 인코딩\n",
    "            dummies_train = pd.get_dummies(X[col], prefix=col, drop_first=False)\n",
    "            X = pd.concat([X.drop(columns=[col]), dummies_train], axis=1)\n",
    "            \n",
    "            # 테스트 데이터 원핫 인코딩\n",
    "            dummies_test = pd.get_dummies(X_test[col], prefix=col, drop_first=False)\n",
    "            X_test = pd.concat([X_test.drop(columns=[col]), dummies_test], axis=1)\n",
    "    \n",
    "    # 건물 번호 원핫 인코딩\n",
    "    X = pd.get_dummies(X, columns=['building_number'], prefix='building', drop_first=False)\n",
    "    X_test = pd.get_dummies(X_test, columns=['building_number'], prefix='building', drop_first=False)\n",
    "    \n",
    "    # 훈련/테스트 데이터 컬럼 맞추기\n",
    "    X, X_test = X.align(X_test, join='outer', axis=1, fill_value=0)\n",
    "    \n",
    "    # NaN/Inf 처리\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    return X, y, X_test\n",
    "\n",
    "# 피처 준비\n",
    "print(\"모델링용 피처 준비 중...\")\n",
    "X, y, X_test = prepare_model_features(train, test)\n",
    "\n",
    "print(f\"최종 피처 수: {X.shape[1]}\")\n",
    "print(f\"훈련 데이터 크기: {X.shape}\")\n",
    "print(f\"테스트 데이터 크기: {X_test.shape}\")\n",
    "\n",
    "# *** 개선사항 6: 시계열 교차검증 (TimeSeriesSplit) ***\n",
    "def time_series_cross_validation(X, y, sample_weights, n_splits=5):\n",
    "    \"\"\"시계열 교차검증을 통한 모델 평가 및 앙상블\"\"\"\n",
    "    \n",
    "    # 시간순 정렬을 위한 인덱스 생성 (원본 날짜 기준)\n",
    "    time_index = train.reset_index()['date_time']\n",
    "    sort_idx = time_index.argsort()\n",
    "    \n",
    "    X_sorted = X.iloc[sort_idx].reset_index(drop=True)\n",
    "    y_sorted = y.iloc[sort_idx].reset_index(drop=True)\n",
    "    weights_sorted = sample_weights.iloc[sort_idx].reset_index(drop=True)\n",
    "    \n",
    "    # TimeSeriesSplit 설정\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=len(X_sorted)//10)\n",
    "    \n",
    "    # 모델별 결과 저장\n",
    "    xgb_scores = []\n",
    "    lgb_scores = []\n",
    "    xgb_predictions = []\n",
    "    lgb_predictions = []\n",
    "    \n",
    "    fold_idx = 0\n",
    "    for train_idx, val_idx in tscv.split(X_sorted):\n",
    "        fold_idx += 1\n",
    "        print(f\"\\n=== Fold {fold_idx} ===\")\n",
    "        \n",
    "        # 데이터 분할\n",
    "        X_train_fold, X_val_fold = X_sorted.iloc[train_idx], X_sorted.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_sorted.iloc[train_idx], y_sorted.iloc[val_idx]\n",
    "        w_train_fold = weights_sorted.iloc[train_idx]\n",
    "        \n",
    "        # 로그 변환 (안전한 변환)\n",
    "        y_train_log = np.log1p(np.clip(y_train_fold, 0, None))\n",
    "        y_val_log = np.log1p(np.clip(y_val_fold, 0, None))\n",
    "        \n",
    "        # XGBoost 모델\n",
    "        xgb_model = XGBRegressor(\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=1000,\n",
    "            max_depth=6,\n",
    "            min_child_weight=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_SEED,\n",
    "            objective=weighted_mse(alpha=1.5),\n",
    "            tree_method='hist',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(\n",
    "            X_train_fold, y_train_log,\n",
    "            sample_weight=w_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_log)],\n",
    "            eval_metric=custom_smape,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # XGBoost 예측\n",
    "        xgb_pred_log = xgb_model.predict(X_val_fold)\n",
    "        xgb_pred = np.expm1(xgb_pred_log)\n",
    "        xgb_score = smape(y_val_fold, xgb_pred)\n",
    "        xgb_scores.append(xgb_score)\n",
    "        \n",
    "        # 테스트 데이터 예측\n",
    "        xgb_test_pred_log = xgb_model.predict(X_test)\n",
    "        xgb_test_pred = np.expm1(xgb_test_pred_log)\n",
    "        xgb_predictions.append(xgb_test_pred)\n",
    "        \n",
    "        # LightGBM 모델 \n",
    "        lgb_model = LGBMRegressor(\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=1000,\n",
    "            max_depth=6,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_SEED,\n",
    "            objective='regression',\n",
    "            metric='mae',\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "        lgb_model.fit(\n",
    "            X_train_fold, y_train_log,\n",
    "            sample_weight=w_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_log)],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        # LightGBM 예측\n",
    "        lgb_pred_log = lgb_model.predict(X_val_fold)\n",
    "        lgb_pred = np.expm1(lgb_pred_log)\n",
    "        lgb_score = smape(y_val_fold, lgb_pred)\n",
    "        lgb_scores.append(lgb_score)\n",
    "        \n",
    "        # 테스트 데이터 예측\n",
    "        lgb_test_pred_log = lgb_model.predict(X_test)\n",
    "        lgb_test_pred = np.expm1(lgb_test_pred_log)\n",
    "        lgb_predictions.append(lgb_test_pred)\n",
    "        \n",
    "        print(f\"Fold {fold_idx} - XGB SMAPE: {xgb_score:.4f}, LGB SMAPE: {lgb_score:.4f}\")\n",
    "    \n",
    "    # 평균 성능\n",
    "    avg_xgb_score = np.mean(xgb_scores)\n",
    "    avg_lgb_score = np.mean(lgb_scores)\n",
    "    \n",
    "    print(f\"\\n=== 최종 교차검증 결과 ===\")\n",
    "    print(f\"XGBoost 평균 SMAPE: {avg_xgb_score:.4f} (±{np.std(xgb_scores):.4f})\")\n",
    "    print(f\"LightGBM 평균 SMAPE: {avg_lgb_score:.4f} (±{np.std(lgb_scores):.4f})\")\n",
    "    \n",
    "    # *** 개선사항 7: 앙상블 (중앙값 사용) ***\n",
    "    xgb_ensemble = np.median(xgb_predictions, axis=0)\n",
    "    lgb_ensemble = np.median(lgb_predictions, axis=0)\n",
    "    \n",
    "    # 최종 앙상블 (XGBoost + LightGBM 중앙값)\n",
    "    final_ensemble = np.median([xgb_ensemble, lgb_ensemble], axis=0)\n",
    "    \n",
    "    return final_ensemble, avg_xgb_score, avg_lgb_score\n",
    "\n",
    "# 시계열 교차검증 실행\n",
    "print(\"\\n시계열 교차검증 및 앙상블 모델링 시작...\")\n",
    "sample_weights = train['sample_weight']\n",
    "\n",
    "final_predictions, xgb_score, lgb_score = time_series_cross_validation(X, y, sample_weights, n_splits=5)\n",
    "\n",
    "print(f\"\\n최종 앙상블 완료\")\n",
    "print(f\"예측 결과 통계:\")\n",
    "print(f\"  - 최소값: {final_predictions.min():.2f}\")\n",
    "print(f\"  - 최대값: {final_predictions.max():.2f}\")\n",
    "print(f\"  - 평균값: {final_predictions.mean():.2f}\")\n",
    "print(f\"  - 중앙값: {np.median(final_predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127811ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 9: 결과 저장 및 검증\n",
    "\n",
    "# 결과 파일 생성\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['answer'] = final_predictions\n",
    "\n",
    "# 예측값 검증\n",
    "print(\"=== 예측 결과 검증 ===\")\n",
    "print(f\"총 예측 샘플 수: {len(final_predictions)}\")\n",
    "print(f\"음수 예측값: {sum(final_predictions < 0)}개\")\n",
    "print(f\"0 예측값: {sum(final_predictions == 0)}개\")\n",
    "print(f\"이상 큰 값 (99.9% 분위수 초과): {sum(final_predictions > np.percentile(final_predictions, 99.9) * 3)}개\")\n",
    "\n",
    "# 음수값이 있다면 0으로 클리핑\n",
    "if sum(final_predictions < 0) > 0:\n",
    "    print(\"음수 예측값을 0으로 클리핑합니다.\")\n",
    "    final_predictions = np.clip(final_predictions, 0, None)\n",
    "    submission['answer'] = final_predictions\n",
    "\n",
    "# 건물별 예측 통계\n",
    "print(\"\\n=== 건물 유형별 예측 통계 ===\")\n",
    "test_with_predictions = test.copy()\n",
    "test_with_predictions['predictions'] = final_predictions\n",
    "\n",
    "building_type_stats = test_with_predictions.groupby('building_type')['predictions'].agg([\n",
    "    'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "]).round(2)\n",
    "\n",
    "print(building_type_stats)\n",
    "\n",
    "# 시간대별 예측 패턴 검증\n",
    "print(\"\\n=== 시간대별 예측 패턴 ===\")\n",
    "test_with_predictions['hour'] = test_with_predictions['date_time'].dt.hour\n",
    "hourly_stats = test_with_predictions.groupby('hour')['predictions'].agg(['mean', 'std']).round(2)\n",
    "print(\"시간별 평균 전력 소비량 (상위 5개/하위 5개 시간대):\")\n",
    "print(\"상위 5개:\", hourly_stats.sort_values('mean', ascending=False).head())\n",
    "print(\"하위 5개:\", hourly_stats.sort_values('mean', ascending=True).head())\n",
    "\n",
    "# 요일별 예측 패턴 검증\n",
    "print(\"\\n=== 요일별 예측 패턴 ===\")\n",
    "test_with_predictions['day_of_week'] = test_with_predictions['date_time'].dt.dayofweek\n",
    "weekday_stats = test_with_predictions.groupby('day_of_week')['predictions'].agg(['mean', 'std']).round(2)\n",
    "weekday_names = ['월', '화', '수', '목', '금', '토', '일']\n",
    "weekday_stats.index = weekday_names\n",
    "print(weekday_stats)\n",
    "\n",
    "# 최종 파일 저장\n",
    "output_filename = f'submission_enhanced_ensemble_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "submission.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n=== 최종 결과 ===\")\n",
    "print(f\"제출 파일 저장: {output_filename}\")\n",
    "print(f\"XGBoost 평균 SMAPE: {xgb_score:.4f}\")\n",
    "print(f\"LightGBM 평균 SMAPE: {lgb_score:.4f}\")\n",
    "print(f\"앙상블 모델 완성!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b6a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
