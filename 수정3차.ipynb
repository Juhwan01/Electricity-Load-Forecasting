{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2606838c",
   "metadata": {},
   "source": [
    "# í•¨ìˆ˜ ì„ ì–¸ ë°  import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527ea83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T12:15:59.117024Z",
     "start_time": "2023-09-01T12:15:59.110018Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import sklearn\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import random as rn\n",
    "\n",
    "RANDOM_SEED = 2024\n",
    "np.random.seed(RANDOM_SEED)\n",
    "rn.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bb3e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T12:16:06.920627Z",
     "start_time": "2023-09-01T12:16:06.916624Z"
    }
   },
   "outputs": [],
   "source": [
    "def smape(gt, preds):\n",
    "    gt= np.array(gt)\n",
    "    preds = np.array(preds)\n",
    "    v = 2 * abs(preds - gt) / (abs(preds) + abs(gt))\n",
    "    score = np.mean(v) * 100\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ed4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T12:16:07.296669Z",
     "start_time": "2023-09-01T12:16:07.280654Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_mse(alpha = 1):\n",
    "    def weighted_mse_fixed(label, pred):\n",
    "        residual = (label - pred).astype(\"float\")\n",
    "        grad = np.where(residual>0, -2*alpha*residual, -2*residual)\n",
    "        hess = np.where(residual>0, 2*alpha, 2.0)\n",
    "        return grad, hess\n",
    "    return weighted_mse_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a94d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T12:16:08.919122Z",
     "start_time": "2023-09-01T12:16:08.904108Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_smape(preds, dtrain):\n",
    "    labels_t = dtrain.get_label()              # log1p ë¼ë²¨\n",
    "    preds_t  = preds                           # log1p ì˜ˆì¸¡\n",
    "    labels_o = np.expm1(labels_t)              # ì› ìŠ¤ì¼€ì¼\n",
    "    preds_o  = np.expm1(preds_t)\n",
    "    denom = np.abs(preds_o) + np.abs(labels_o)\n",
    "    smape_val = 100.0 * np.mean((2.0 * np.abs(preds_o - labels_o)) / np.maximum(denom, 1e-9))\n",
    "    return 'custom_smape', float(smape_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c93aad7",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59754f5d",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e49868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:54.861486Z",
     "start_time": "2023-09-01T09:55:54.703696Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "building_info = pd.read_csv('building_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddf770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:54.966582Z",
     "start_time": "2023-09-01T09:55:54.940559Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train.rename(columns={\n",
    "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
    "    'ì¼ì‹œ': 'date_time',\n",
    "    'ê¸°ì˜¨(Â°C)': 'temperature',\n",
    "    'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',\n",
    "    'í’ì†(m/s)': 'windspeed',\n",
    "    'ìŠµë„(%)': 'humidity',\n",
    "    'ì¼ì¡°(hr)': 'sunshine',\n",
    "    'ì¼ì‚¬(MJ/m2)': 'solar_radiation',\n",
    "    'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'power_consumption'\n",
    "})\n",
    "train.drop('num_date_time', axis = 1, inplace=True)\n",
    "\n",
    "test = test.rename(columns={\n",
    "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
    "    'ì¼ì‹œ': 'date_time',\n",
    "    'ê¸°ì˜¨(Â°C)': 'temperature',\n",
    "    'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',\n",
    "    'í’ì†(m/s)': 'windspeed',\n",
    "    'ìŠµë„(%)': 'humidity',\n",
    "    'ì¼ì¡°(hr)': 'sunshine',\n",
    "    'ì¼ì‚¬(MJ/m2)': 'solar_radiation',\n",
    "    'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'power_consumption'\n",
    "})\n",
    "test.drop('num_date_time', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0d646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:55.116716Z",
     "start_time": "2023-09-01T09:55:55.108709Z"
    }
   },
   "outputs": [],
   "source": [
    "building_info = building_info.rename(columns={\n",
    "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
    "    'ê±´ë¬¼ìœ í˜•': 'building_type',\n",
    "    'ì—°ë©´ì (m2)': 'total_area',\n",
    "    'ëƒ‰ë°©ë©´ì (m2)': 'cooling_area',\n",
    "    'íƒœì–‘ê´‘ìš©ëŸ‰(kW)': 'solar_power_capacity',\n",
    "    'ESSì €ì¥ìš©ëŸ‰(kWh)': 'ess_capacity',\n",
    "    'PCSìš©ëŸ‰(kW)': 'pcs_capacity'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36f788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:55.281867Z",
     "start_time": "2023-09-01T09:55:55.263850Z"
    }
   },
   "outputs": [],
   "source": [
    "translation_dict = {\n",
    "    'ê±´ë¬¼ê¸°íƒ€': 'Other Buildings',\n",
    "    'ê³µê³µ': 'Public',\n",
    "    'ëŒ€í•™êµ': 'University',\n",
    "    'ë°ì´í„°ì„¼í„°': 'Data Center',\n",
    "    'ë°±í™”ì ë°ì•„ìš¸ë ›': 'Department Store and Outlet',\n",
    "    'ë³‘ì›': 'Hospital',\n",
    "    'ìƒìš©': 'Commercial',\n",
    "    'ì•„íŒŒíŠ¸': 'Apartment',\n",
    "    'ì—°êµ¬ì†Œ': 'Research Institute',\n",
    "    'ì§€ì‹ì‚°ì—…ì„¼í„°': 'Knowledge Industry Center',\n",
    "    'í• ì¸ë§ˆíŠ¸': 'Discount Mart',\n",
    "    'í˜¸í…”ë°ë¦¬ì¡°íŠ¸': 'Hotel and Resort'\n",
    "}\n",
    "\n",
    "building_info['building_type'] = building_info['building_type'].replace(translation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088cd6cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:55.432003Z",
     "start_time": "2023-09-01T09:55:55.418991Z"
    }
   },
   "outputs": [],
   "source": [
    "building_info['solar_power_utility'] = np.where(building_info.solar_power_capacity !='-',1,0)\n",
    "building_info['ess_utility'] = np.where(building_info.ess_capacity !='-',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef2583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:55.597153Z",
     "start_time": "2023-09-01T09:55:55.569128Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, building_info, on='building_number', how='left')\n",
    "test = pd.merge(test, building_info, on='building_number', how='left')\n",
    "\n",
    "# ğŸ”¹ ì—¬ê¸°ì„œ ëƒ‰ë°©ë©´ì ë¹„ ì¶”ê°€\n",
    "train[\"cooling_ratio\"] = (\n",
    "    train[\"cooling_area\"] / train[\"total_area\"]\n",
    ").replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "test[\"cooling_ratio\"] = (\n",
    "    test[\"cooling_area\"] / test[\"total_area\"]\n",
    ").replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b38f22",
   "metadata": {},
   "source": [
    "## ê²°ì¸¡ì¹˜ í™•ì¸ ë° ë³´ê°„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a6966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:56.422564Z",
     "start_time": "2023-09-01T09:55:56.389534Z"
    }
   },
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88de08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:56.857370Z",
     "start_time": "2023-09-01T09:55:56.835349Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.solar_power_capacity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4059d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:57.172845Z",
     "start_time": "2023-09-01T09:55:57.161836Z"
    }
   },
   "outputs": [],
   "source": [
    "train.ess_capacity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70841b49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:57.368025Z",
     "start_time": "2023-09-01T09:55:57.351010Z"
    }
   },
   "outputs": [],
   "source": [
    "train.pcs_capacity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['windspeed']= train.windspeed.interpolate()\n",
    "train['humidity']= train.humidity.interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebba91d",
   "metadata": {},
   "source": [
    "## Datetime ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48c18d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:55:58.103624Z",
     "start_time": "2023-09-01T09:55:57.991522Z"
    }
   },
   "outputs": [],
   "source": [
    "train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "# date time feature ìƒì„±\n",
    "train['hour'] = train['date_time'].dt.hour\n",
    "train['day'] = train['date_time'].dt.day\n",
    "train['month'] = train['date_time'].dt.month\n",
    "train['day_of_week'] = train['date_time'].dt.dayofweek #ìš”ì¼\n",
    "\n",
    "\n",
    "test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "# date time feature ìƒì„±\n",
    "test['hour'] = test['date_time'].dt.hour\n",
    "test['day'] = test['date_time'].dt.day\n",
    "test['month'] = test['date_time'].dt.month\n",
    "test['day_of_week'] = test['date_time'].dt.dayofweek #ìš”ì¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc4133",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342c005",
   "metadata": {},
   "source": [
    "### í‰ê· ê¸°ì˜¨, ìµœëŒ€ê¸°ì˜¨ ë³€ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790fec52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:20.587052Z",
     "start_time": "2023-09-01T09:55:58.499939Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_day_values(dataframe, target_column, output_column, aggregation_func):\n",
    "    result_dict = {}\n",
    "\n",
    "    grouped_temp = dataframe.groupby(['building_number', 'month', 'day'])[target_column].agg(aggregation_func)\n",
    "\n",
    "    for (building, month, day), value in grouped_temp.items():\n",
    "        result_dict.setdefault(building, {}).setdefault(month, {})[day] = value\n",
    "\n",
    "    dataframe[output_column] = [\n",
    "        result_dict.get(row['building_number'], {}).get(row['month'], {}).get(row['day'], None)\n",
    "        for _, row in dataframe.iterrows()\n",
    "    ]\n",
    "\n",
    "    \n",
    "train['day_max_temperature'] = 0.0\n",
    "train['day_mean_temperature'] = 0.0\n",
    "\n",
    "calculate_day_values(train, 'temperature', 'day_max_temperature', 'max')\n",
    "calculate_day_values(train, 'temperature', 'day_mean_temperature', 'mean')\n",
    "calculate_day_values(train, 'temperature', 'day_min_temperature', 'min')\n",
    "\n",
    "train['day_temperature_range'] = train['day_max_temperature'] - train['day_min_temperature']\n",
    "\n",
    "calculate_day_values(test, 'temperature', 'day_max_temperature', 'max')\n",
    "calculate_day_values(test, 'temperature', 'day_mean_temperature', 'mean')\n",
    "calculate_day_values(test, 'temperature', 'day_min_temperature', 'min')\n",
    "\n",
    "test['day_temperature_range'] = test['day_max_temperature'] - test['day_min_temperature']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432f9f8",
   "metadata": {},
   "source": [
    "### Outlier drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a4e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:20.617080Z",
     "start_time": "2023-09-01T09:56:20.588053Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# =========================================\n",
    "# 0) ìœ í‹¸: ì•ˆì „í•œ zscore (robust: median/MAD)\n",
    "# =========================================\n",
    "def robust_zscore(x):\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med)) + 1e-9\n",
    "    return (x - med) / (1.4826 * mad)\n",
    "\n",
    "# =========================================\n",
    "# 1) ê·œì¹™ ê¸°ë°˜: ì—°ì† 0(í˜¹ì€ ê±°ì˜ 0) runs\n",
    "# =========================================\n",
    "def detect_consecutive_runs(df, col='power_consumption', run_len=4, thresh=1e-6):\n",
    "    \"\"\"\n",
    "    ê±´ë¬¼ë³„ë¡œ date_time ê¸°ì¤€ ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •.\n",
    "    run_len ì‹œê°„ ì´ìƒ ì—°ì†ìœ¼ë¡œ col <= thresh ì¸ êµ¬ê°„ì˜ 'í•´ë‹¹ êµ¬ê°„ ì¸ë±ìŠ¤'ë¥¼ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    bad_idx = []\n",
    "    for b, bdf in df.groupby('building_number'):\n",
    "        bdf = bdf.sort_values('date_time')\n",
    "        mask = (bdf[col] <= thresh).astype(int)\n",
    "        # ì—°ì† êµ¬ê°„ ì°¾ê¸°\n",
    "        # run_idëŠ” ì—°ì† êµ¬ê°„ë§ˆë‹¤ ê°™ì€ ë²ˆí˜¸ë¥¼ ê°–ë„ë¡ ë³€í™˜\n",
    "        changes = mask.diff().fillna(0) != 0\n",
    "        run_id = changes.cumsum()\n",
    "        # ê° run_idë³„ ê¸¸ì´ ê³„ì‚° (0ì¸ êµ¬ê°„ë„ í¬í•¨ë˜ë¯€ë¡œ mask==1ì¸ êµ¬ê°„ë§Œ ê³ ë ¤)\n",
    "        runs = bdf.assign(run_id=run_id)\n",
    "        runs = runs[mask == 1]\n",
    "        if runs.empty:\n",
    "            continue\n",
    "        lengths = runs.groupby('run_id').size()\n",
    "        long_runs = lengths[lengths >= run_len].index\n",
    "        out = runs[runs['run_id'].isin(long_runs)].index.tolist()\n",
    "        bad_idx.extend(out)\n",
    "    return bad_idx\n",
    "\n",
    "# =========================================\n",
    "# 2) STL ë¶„í•´ â†’ ì”ì°¨ ê³„ì‚°\n",
    "# =========================================\n",
    "def stl_residuals_per_building(df, col='power_consumption', period=24):\n",
    "    \"\"\"\n",
    "    ê±´ë¬¼ë³„ë¡œ ì‹œê³„ì—´ì„ STL ë¶„í•´í•˜ê³  residualì„ ë°˜í™˜\n",
    "    period=24 (ì¼ì£¼ê¸°), ì£¼ì¤‘/ì£¼ë§ê¹Œì§€ ê°•í•˜ë©´ 24*7 ê³ ë ¤\n",
    "    \"\"\"\n",
    "    residual = pd.Series(index=df.index, dtype=float)\n",
    "    for b, bdf in df.groupby('building_number'):\n",
    "        bdf = bdf.sort_values('date_time')\n",
    "        y = bdf[col].astype(float).values\n",
    "        # ê¸¸ì´ê°€ period*2 ì´ìƒì´ì–´ì•¼ STL ì•ˆì •ì \n",
    "        if len(y) < period * 2:\n",
    "            residual.loc[bdf.index] = np.nan\n",
    "            continue\n",
    "        try:\n",
    "            stl = STL(y, period=period, robust=True)\n",
    "            res = stl.fit()\n",
    "            resid = y - (res.trend + res.seasonal)\n",
    "            residual.loc[bdf.index] = resid\n",
    "        except Exception:\n",
    "            residual.loc[bdf.index] = np.nan\n",
    "    return residual\n",
    "\n",
    "# =========================================\n",
    "# 3) Hampel í•„í„° (ì”ì°¨ ëŒ€ìƒ)\n",
    "# =========================================\n",
    "def hampel_outliers(series, window=24, n_sigma=3.0):\n",
    "    \"\"\"\n",
    "    ë¡¤ë§ ì¤‘ì•™ê°’/ MAD ê¸°ë°˜ Hampel: |x - med| / (1.4826*MAD) > n_sigma ì´ë©´ ì´ìƒì¹˜\n",
    "    windowëŠ” ê³„ì ˆì£¼ê¸°(24) ë˜ëŠ” ê·¸ ë°°ìˆ˜ ì¶”ì²œ\n",
    "    \"\"\"\n",
    "    x = series.astype(float).copy()\n",
    "    # pandasì˜ rolling median/MAD\n",
    "    med = x.rolling(window, center=True, min_periods=window//2).median()\n",
    "    abs_dev = (x - med).abs()\n",
    "    mad = abs_dev.rolling(window, center=True, min_periods=window//2).median()\n",
    "    z = (x - med) / (1.4826 * (mad + 1e-9))\n",
    "    return series.index[np.abs(z) > n_sigma].tolist()\n",
    "\n",
    "# =========================================\n",
    "# 4) ì”ì°¨ì— robust z-score (ê°„ë‹¨ ESD ëŒ€ìš©)\n",
    "# =========================================\n",
    "def residual_zscore_outliers(series, z_thresh=4.0):\n",
    "    z = robust_zscore(series.values)\n",
    "    return series.index[np.abs(z) > z_thresh].tolist()\n",
    "\n",
    "\n",
    "# === (ADD) ì™„í™”í˜•: ì´ë²¤íŠ¸ì„±ì€ ì‚´ë¦¬ê³  ì •ì „/ì„¼ì„œ ì˜¤ë¥˜ë§Œ ë“œë¡­ ===\n",
    "RELAX_TYPES = [\"Public\", \"Other Buildings\", \"í˜¸í…”\", \"ë°±í™”ì \"]\n",
    "\n",
    "def build_outlier_list_relaxed(train,\n",
    "                               value_col='power_consumption',\n",
    "                               run_len=4,\n",
    "                               zero_thresh=1e-6,\n",
    "                               stl_period=24,\n",
    "                               hampel_window=24,\n",
    "                               hampel_sigma_relax=5.0,  # ì™„í™” íƒ€ì…\n",
    "                               hampel_sigma_base=3.0,   # ì¼ë°˜ íƒ€ì…\n",
    "                               resid_z_relax=7.0,       # ì™„í™” íƒ€ì…\n",
    "                               resid_z_base=4.0,        # ì¼ë°˜ íƒ€ì…\n",
    "                               use_weekly=False):\n",
    "    \"\"\"\n",
    "    ë°˜í™˜: (drop_idx, extreme_idx)\n",
    "      - drop_idx: ì‹¤ì œ ì‚­ì œ(ì •ì „/ì„¼ì„œì˜¤ë¥˜ + (ì™„í™” ì•„ë‹˜ íƒ€ì…ì˜ ì´ë²¤íŠ¸ì„±))\n",
    "      - extreme_idx: ì‚­ì œ ì•ˆ í•˜ê³  is_extreme=1ë¡œ í‘œì‹œ(ì™„í™” íƒ€ì…ì˜ ì´ë²¤íŠ¸ì„±)\n",
    "    \"\"\"\n",
    "    df = train.copy()\n",
    "    assert 'building_type' in df.columns, \"trainì— building_type í•„ìš”\"\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df = df.sort_values(['building_type','building_number','date_time'])\n",
    "\n",
    "    drop_idx, extreme_idx = set(), set()\n",
    "\n",
    "    # ì „ì²´ ìƒìœ„ ê°€ë“œ(ë§ë„ ì•ˆë˜ê²Œ í° ê°’ ì»·)\n",
    "    try:\n",
    "        absurd_hi = np.nanpercentile(df[value_col], 99.9) * 5.0\n",
    "        if not np.isfinite(absurd_hi): absurd_hi = None\n",
    "    except Exception:\n",
    "        absurd_hi = None\n",
    "\n",
    "    for btype, g in df.groupby('building_type'):\n",
    "        # 1) ì—°ì† 0(run) â†’ ì •ì „/ì„¼ì„œì˜¤ë¥˜: ë¬´ì¡°ê±´ drop\n",
    "        o1 = detect_consecutive_runs(g, col=value_col, run_len=run_len, thresh=zero_thresh)\n",
    "\n",
    "        # 2) STL ì”ì°¨ ê¸°ë°˜ ì´ë²¤íŠ¸ì„± í›„ë³´\n",
    "        resid_daily = stl_residuals_per_building(g, col=value_col, period=stl_period)\n",
    "        if btype in RELAX_TYPES:\n",
    "            hs, rz = hampel_sigma_relax, resid_z_relax\n",
    "        else:\n",
    "            hs, rz = hampel_sigma_base, resid_z_base\n",
    "\n",
    "        o2 = hampel_outliers(resid_daily, window=hampel_window, n_sigma=hs)\n",
    "        o3 = residual_zscore_outliers(resid_daily.dropna(), z_thresh=rz)\n",
    "\n",
    "        if use_weekly:\n",
    "            resid_week = stl_residuals_per_building(g, col=value_col, period=24*7)\n",
    "            o4 = hampel_outliers(resid_week, window=24*7, n_sigma=hs)\n",
    "            o5 = residual_zscore_outliers(resid_week.dropna(), z_thresh=rz)\n",
    "        else:\n",
    "            o4, o5 = [], []\n",
    "\n",
    "        event_like  = set(o2) | set(o3) | set(o4) | set(o5)\n",
    "\n",
    "        # 3) ì„¼ì„œ/ì •ì „ ì§ì ‘ ê·œì¹™(0, ìŒìˆ˜, ê¸‰ë½, í™©ë‹¹íˆ í° ê°’)\n",
    "        sub = g[value_col].astype(float)\n",
    "        outage_mask = (sub <= zero_thresh) | (sub < 0)\n",
    "        if absurd_hi is not None:\n",
    "            outage_mask |= (sub > absurd_hi)\n",
    "        prev = sub.shift(1)\n",
    "        drop_mask = (prev > 0) & ((prev - sub) / (prev + 1e-9) >= 0.95)\n",
    "        sensor_like = set(g.index[outage_mask | drop_mask])\n",
    "\n",
    "        if btype in RELAX_TYPES:\n",
    "            # ì™„í™” íƒ€ì…: ì„¼ì„œëŠ” drop, ì´ë²¤íŠ¸ì„±ì€ ì‚´ë ¤ì„œ í”Œë˜ê·¸\n",
    "            drop_idx |= sensor_like\n",
    "            extreme_idx |= (event_like - sensor_like)\n",
    "        else:\n",
    "            # ì¼ë°˜ íƒ€ì…: ì´ë²¤íŠ¸ì„±ë„ drop\n",
    "            drop_idx |= (sensor_like | event_like)\n",
    "\n",
    "        # (ì„ íƒ) ê³¼ë„ ì œê±° ë°©ì§€: ê±´ë¬¼ë³„ 2%\n",
    "        for bn, bdf in g.groupby('building_number'):\n",
    "            b_out = list(set(bdf.index) & drop_idx)\n",
    "            max_keep = int(0.02 * len(bdf))\n",
    "            if len(b_out) > max_keep and max_keep > 0:\n",
    "                keep_back = set(bdf.index) & drop_idx\n",
    "                # ë‹¨ìˆœ ìƒí•œ ì ìš©(ì›í•˜ë©´ |ì”ì°¨| ê¸°ì¤€ ì •ë ¬í•´ë„ ë¨)\n",
    "                keep_back = set(list(keep_back)[:max_keep])\n",
    "                drop_idx -= ((set(bdf.index) & drop_idx) - keep_back)\n",
    "\n",
    "    return sorted(list(drop_idx)), sorted(list(extreme_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx, extreme_idx = build_outlier_list_relaxed(\n",
    "    train,\n",
    "    value_col='power_consumption',\n",
    "    run_len=4,\n",
    "    zero_thresh=1e-6,\n",
    "    stl_period=24,\n",
    "    hampel_window=24,\n",
    "    hampel_sigma_relax=5.0,  # ì™„í™”\n",
    "    hampel_sigma_base=3.0,   # ê¸°ë³¸\n",
    "    resid_z_relax=7.0,       # ì™„í™”\n",
    "    resid_z_base=4.0,        # ê¸°ë³¸\n",
    "    use_weekly=True\n",
    ")\n",
    "print(f\"ì‚­ì œ(drop): {len(drop_idx)} | ì´ë²¤íŠ¸ì„± í‘œì‹œ(extreme): {len(extreme_idx)}\")\n",
    "\n",
    "# í”Œë˜ê·¸ ì¶”ê°€ + ë“œë¡­ ì ìš©\n",
    "train_marked = train.copy()\n",
    "train_marked[\"is_extreme\"] = 0\n",
    "train_marked.loc[extreme_idx, \"is_extreme\"] = 1\n",
    "\n",
    "train = train_marked.drop(index=drop_idx).reset_index(drop=True)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì—ëŠ” í”Œë˜ê·¸ 0ìœ¼ë¡œ ìƒì„±\n",
    "if \"is_extreme\" not in test.columns:\n",
    "    test[\"is_extreme\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba59e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T07:59:24.686619Z",
     "start_time": "2023-08-18T07:59:24.676611Z"
    }
   },
   "source": [
    "### ì„ì‹œ íœ´ë¬´ ì¶”ì¸¡ ë°ì´í„° drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c71db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:21.742893Z",
     "start_time": "2023-09-01T09:56:20.618081Z"
    }
   },
   "outputs": [],
   "source": [
    "# temp_hol = {2 : ['2022-06-17'], \n",
    "#     5 : ['2022-07-25','2022-08-02','2022-08-09','2022-08-16'],\n",
    "#     11 : ['2022-06-17'], 12 : ['2022-07-02'], 17 : ['2022-06-18','2022-07-25'],\n",
    "#     21 : ['2022-07-01','2022-07-03','2022-07-17','2022-07-30'], \n",
    "#     37 : ['2022-06-20','2022-07-11','2022-08-08'], \n",
    "#     38 : ['2022-06-13','2022-07-25','2022-08-01'],\n",
    "#     39 : ['2022-07-18','2022-08-08'],\n",
    "#     40 : ['2022-06-20','2022-07-18','2022-08-08'],\n",
    "#     41 : ['2022-06-27','2022-07-25','2022-08-08'],\n",
    "#     42 : ['2022-06-13','2022-07-11','2022-08-22'],\n",
    "#     54 : ['2022-08-16','2022-08-17'],74 : ['2022-06-03'],\n",
    "#     75 : ['2022-06-15','2022-06-17','2022-06-20','2022-06-21'],\n",
    "#     86 : ['2022-06-10','2022-08-10'],\n",
    "#     89 : ['2022-07-09'], 91 : ['2022-06-13','2022-07-11','2022-08-22','2022-06-08'], 92 : ['2022-07-30']}\n",
    "\n",
    "\n",
    "# mask = train.apply(lambda x: x['building_number'] in temp_hol and str(x['date_time'])[:10] in temp_hol[x['building_number']], axis=1)\n",
    "\n",
    "# train.drop(train[mask].index, axis=0, inplace=True)\n",
    "\n",
    "# train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af13843",
   "metadata": {},
   "source": [
    "### ê³µíœ´ì¼ë³€ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "holi_weekday = [\n",
    "    '2024-06-06',  # í˜„ì¶©ì¼ (ëª©)\n",
    "    '2024-08-15'   # ê´‘ë³µì ˆ (ëª©)\n",
    "]\n",
    "\n",
    "train['holiday'] = np.where((train.day_of_week >= 5) | (train.date_time.dt.strftime('%Y-%m-%d').isin(holi_weekday)), 1, 0)\n",
    "test['holiday'] = np.where((test.day_of_week >= 5) | (test.date_time.dt.strftime('%Y-%m-%d').isin(holi_weekday)), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ae359",
   "metadata": {},
   "source": [
    "### ëŒ€í˜•ë§ˆíŠ¸ íœ´ë¬´ì¼ìš”ì¼ ë³€ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4452ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "holi_sun = [\n",
    "    '2024-06-09', '2024-06-23',  # 6ì›”\n",
    "    '2024-07-14', '2024-07-28',  # 7ì›”\n",
    "    '2024-08-11', '2024-08-25'   # 8ì›”\n",
    "]\n",
    "\n",
    "# ì˜ë¬´ íœ´ì—… ì¼ìš”ì¼ì´ë©´ 1, ì•„ë‹ˆë©´ 0\n",
    "train['Sunday_holiday'] = np.where((train.day_of_week == 6) & (train.date_time.dt.strftime('%Y-%m-%d').isin(holi_sun)), 1, 0)\n",
    "test['Sunday_holiday'] = np.where((test.day_of_week == 6) & (test.date_time.dt.strftime('%Y-%m-%d').isin(holi_sun)), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830eb712",
   "metadata": {},
   "source": [
    "### ì‹œê°„ë³€ìˆ˜ í‘¸ë¦¬ì—ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e2c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:21.998125Z",
     "start_time": "2023-09-01T09:56:21.954085Z"
    }
   },
   "outputs": [],
   "source": [
    "#ì‹œê°„\n",
    "train['sin_hour'] = np.sin(2 * np.pi * train['hour']/23.0)\n",
    "train['cos_hour'] = np.cos(2 * np.pi * train['hour']/23.0)\n",
    "test['sin_hour'] = np.sin(2 * np.pi * test['hour']/23.0)\n",
    "test['cos_hour'] = np.cos(2 * np.pi * test['hour']/23.0)\n",
    "\n",
    "#ë‚ ì§œ\n",
    "train['sin_date'] = -np.sin(2 * np.pi * (train['month']+train['day']/31)/12)\n",
    "train['cos_date'] = -np.cos(2 * np.pi * (train['month']+train['day']/31)/12)\n",
    "test['sin_date'] = -np.sin(2 * np.pi * (test['month']+test['day']/31)/12)\n",
    "test['cos_date'] = -np.cos(2 * np.pi * (test['month']+test['day']/31)/12)\n",
    "\n",
    "#ì›”\n",
    "train['sin_month'] = -np.sin(2 * np.pi * train['month']/12.0)\n",
    "train['cos_month'] = -np.cos(2 * np.pi * train['month']/12.0)\n",
    "test['sin_month'] = -np.sin(2 * np.pi * test['month']/12.0)\n",
    "test['cos_month'] = -np.cos(2 * np.pi * test['month']/12.0)\n",
    "\n",
    "#ìš”ì¼\n",
    "train['sin_dayofweek'] = -np.sin(2 * np.pi * (train['day_of_week']+1)/7.0)\n",
    "train['cos_dayofweek'] = -np.cos(2 * np.pi * (train['day_of_week']+1)/7.0)\n",
    "test['sin_dayofweek'] = -np.sin(2 * np.pi * (test['day_of_week']+1)/7.0)\n",
    "test['cos_dayofweek'] = -np.cos(2 * np.pi * (test['day_of_week']+1)/7.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04681b67",
   "metadata": {},
   "source": [
    "### CDH(ëƒ‰ë°©ë„ì‹œ) ë³€ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ebec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:22.133621Z",
     "start_time": "2023-09-01T09:56:21.999126Z"
    }
   },
   "outputs": [],
   "source": [
    "def CDH(xs):\n",
    "    cumsum = np.cumsum(xs - 26)\n",
    "    return np.concatenate((cumsum[:11], cumsum[11:] - cumsum[:-11]))\n",
    "\n",
    "def calculate_and_add_cdh(dataframe):\n",
    "    cdhs = []\n",
    "    for i in range(1, 101):\n",
    "        temp = dataframe[dataframe['building_number'] == i]['temperature'].values\n",
    "        cdh = CDH(temp)\n",
    "        cdhs.append(cdh)\n",
    "    return np.concatenate(cdhs)\n",
    "\n",
    "train['CDH'] = calculate_and_add_cdh(train)\n",
    "test['CDH'] = calculate_and_add_cdh(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2928f6",
   "metadata": {},
   "source": [
    "### THI(ë¶ˆì¾Œì§€ìˆ˜) ë³€ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b733555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:22.148635Z",
     "start_time": "2023-09-01T09:56:22.133621Z"
    }
   },
   "outputs": [],
   "source": [
    "train['THI'] = 9/5*train['temperature'] - 0.55*(1-train['humidity']/100)*(9/5*train['humidity']-26)+32\n",
    "\n",
    "test['THI'] = 9/5*test['temperature'] - 0.55*(1-test['humidity']/100)*(9/5*test['humidity']-26)+32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e6e19",
   "metadata": {},
   "source": [
    "### WCT(ì²´ê°ì˜¨ë„) ë³€ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdf79c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:22.163649Z",
     "start_time": "2023-09-01T09:56:22.149636Z"
    }
   },
   "outputs": [],
   "source": [
    "train['WCT'] = 13.12 + 0.6125*train['temperature'] - 11.37*(train['windspeed']**\n",
    "                                                            0.16) + 0.3965*(train['windspeed']**0.16)*train['temperature']\n",
    "test['WCT'] = 13.12 + 0.6125*test['temperature'] - 11.37*(test['windspeed']**\n",
    "                                                            0.16) + 0.3965*(test['windspeed']**0.16)*test['temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1ab63",
   "metadata": {},
   "source": [
    "### ì „ë ¥ì†Œë¹„ í†µê³„ëŸ‰ ë³€ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c6408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:22.403867Z",
     "start_time": "2023-09-01T09:56:22.164649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate 'day_hour_mean'\n",
    "power_mean = pd.pivot_table(train, values='power_consumption', index=['building_number', 'hour', 'day_of_week'], aggfunc=np.mean).reset_index()\n",
    "power_mean.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_mean']\n",
    "\n",
    "# Calculate 'day_hour_std'\n",
    "power_std = pd.pivot_table(train, values='power_consumption', index=['building_number', 'hour', 'day_of_week'], aggfunc=np.std).reset_index()\n",
    "power_std.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_std']\n",
    "\n",
    "# Calculate 'hour_mean'\n",
    "power_hour_mean = pd.pivot_table(train, values='power_consumption', index=['building_number', 'hour'], aggfunc=np.mean).reset_index()\n",
    "power_hour_mean.columns = ['building_number', 'hour', 'hour_mean']\n",
    "\n",
    "# Calculate 'hour_std'\n",
    "power_hour_std = pd.pivot_table(train, values='power_consumption', index=['building_number', 'hour'], aggfunc=np.std).reset_index()\n",
    "power_hour_std.columns = ['building_number', 'hour', 'hour_std']\n",
    "\n",
    "# Merge calculated features to 'train' and 'test' dataframes\n",
    "train = train.merge(power_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "test = test.merge(power_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "\n",
    "train = train.merge(power_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "test = test.merge(power_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "\n",
    "train = train.merge(power_hour_mean, on=['building_number', 'hour'], how='left')\n",
    "test = test.merge(power_hour_mean, on=['building_number', 'hour'], how='left')\n",
    "\n",
    "train = train.merge(power_hour_std, on=['building_number', 'hour'], how='left')\n",
    "test = test.merge(power_hour_std, on=['building_number', 'hour'], how='left')\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8f9f2",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ë§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2598b0",
   "metadata": {},
   "source": [
    "## X,Y,test ì„ ì–¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32510f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T09:56:22.418880Z",
     "start_time": "2023-09-01T09:56:22.404867Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop(['solar_power_capacity', 'ess_capacity', 'pcs_capacity',\n",
    "                'power_consumption','rainfall', 'sunshine', 'solar_radiation',\n",
    "                'hour','day','month','day_of_week','date_time'],axis =1 )\n",
    "\n",
    "Y = train[['building_type','power_consumption']]\n",
    "\n",
    "test_X = test.drop(['solar_power_capacity', 'ess_capacity', 'pcs_capacity','rainfall',\n",
    "                   'hour','month','day_of_week','day','date_time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b195ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T12:07:56.459743Z",
     "start_time": "2023-09-01T12:07:56.428037Z"
    }
   },
   "outputs": [],
   "source": [
    "type_list = []\n",
    "for value in train.building_type.values:\n",
    "    if value not in type_list:\n",
    "        type_list.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628caf04",
   "metadata": {},
   "source": [
    "## XGB ê±´ë¬¼ ìœ í˜•ë³„ ë‹¨ì¼ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c995f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T12:07:57.662804Z",
     "start_time": "2023-09-01T12:07:57.656785Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_best_params = pd.read_csv('xgb_best_params_found.csv')\n",
    "xgb_best_params['building_type'] = type_list\n",
    "xgb_best_params.set_index('building_type',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae3886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T12:07:58.803898Z",
     "start_time": "2023-09-01T12:07:58.786884Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸€ë¡œë²Œ ëª¨ë¸ íŒŒë¼ë¯¸í„° (ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµí•  ëª¨ë¸)\n",
    "global_xgb_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 5000,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'alpha': 1.5,  # weighted_mse íŒŒë¼ë¯¸í„°\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "# ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì • (íƒ€ì…ë³„:ê¸€ë¡œë²Œ = 7:3 ë¹„ìœ¨)\n",
    "ENSEMBLE_WEIGHT_TYPE = 0.7  # íƒ€ì…ë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜\n",
    "ENSEMBLE_WEIGHT_GLOBAL = 0.3  # ê¸€ë¡œë²Œ ëª¨ë¸ ê°€ì¤‘ì¹˜\n",
    "\n",
    "print(f\"ì•™ìƒë¸” ê°€ì¤‘ì¹˜ - íƒ€ì…ë³„ ëª¨ë¸: {ENSEMBLE_WEIGHT_TYPE}, ê¸€ë¡œë²Œ ëª¨ë¸: {ENSEMBLE_WEIGHT_GLOBAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f4175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T11:40:03.076892Z",
     "start_time": "2023-08-30T10:49:46.245401Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=7, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© DataFrame\n",
    "answer_df = pd.DataFrame(columns=['answer'])\n",
    "pred_df = pd.DataFrame(columns=['pred'])\n",
    "\n",
    "# ê¸€ë¡œë²Œ ëª¨ë¸ ê²°ê³¼ ì €ì¥ìš©\n",
    "global_answer_df = pd.DataFrame(columns=['answer'])\n",
    "global_pred_df = pd.DataFrame(columns=['pred'])\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"1ë‹¨ê³„: ê¸€ë¡œë²Œ ëª¨ë¸ í•™ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== ê¸€ë¡œë²Œ ëª¨ë¸ í•™ìŠµ =====\n",
    "X_global = X.copy()\n",
    "Y_global = Y['power_consumption'].astype(float).copy()\n",
    "X_test_global = test_X.copy()\n",
    "\n",
    "# ê¸€ë¡œë²Œ ëª¨ë¸ìš© ë°ì´í„° ì •ë¦¬\n",
    "Y_global = Y_global.replace([np.inf, -np.inf], np.nan)\n",
    "ok_global = Y_global.notna() & (Y_global >= 0)\n",
    "if not ok_global.all():\n",
    "    X_global = X_global.loc[ok_global]\n",
    "    Y_global = Y_global.loc[ok_global]\n",
    "\n",
    "# ì›í•« ì¸ì½”ë”©\n",
    "X_global = pd.get_dummies(X_global, columns=['building_number', 'building_type'], drop_first=False)\n",
    "X_test_global = pd.get_dummies(X_test_global, columns=['building_number', 'building_type'], drop_first=False)\n",
    "\n",
    "# train/test ì»¬ëŸ¼ ì •ë ¬\n",
    "X_global, X_test_global = X_global.align(X_test_global, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "# NaN/Inf ì²˜ë¦¬\n",
    "X_global = X_global.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test_global = X_test_global.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ë°ì´í„° íƒ€ì… ì •ë¦¬\n",
    "X_global = X_global.astype(np.float32)\n",
    "Y_global = Y_global.astype(np.float32)\n",
    "\n",
    "# ê¸€ë¡œë²Œ ëª¨ë¸ êµì°¨ê²€ì¦\n",
    "global_fold_smape = []\n",
    "global_answer_list = []\n",
    "global_pred = pd.DataFrame(index=Y_global.index.copy(), columns=['pred'])\n",
    "\n",
    "j = 0\n",
    "for train_index, valid_index in kf.split(X_global):\n",
    "    j += 1\n",
    "    print(f\"ê¸€ë¡œë²Œ ëª¨ë¸ - Fold {j}/{kf.n_splits}\")\n",
    "    \n",
    "    X_train, X_valid = X_global.iloc[train_index], X_global.iloc[valid_index]\n",
    "    Y_train, Y_valid = Y_global.iloc[train_index], Y_global.iloc[valid_index]\n",
    "    \n",
    "    # log1p ë³€í™˜\n",
    "    Y_train_t = np.log1p(np.clip(Y_train, a_min=0, a_max=None))\n",
    "    Y_valid_t = np.log1p(np.clip(Y_valid, a_min=0, a_max=None))\n",
    "    \n",
    "    evals = [(X_train, Y_train_t), (X_valid, Y_valid_t)]\n",
    "    \n",
    "    global_model = XGBRegressor(\n",
    "        learning_rate=global_xgb_params['learning_rate'],\n",
    "        n_estimators=global_xgb_params['n_estimators'],\n",
    "        max_depth=global_xgb_params['max_depth'],\n",
    "        random_state=global_xgb_params['random_state'],\n",
    "        subsample=global_xgb_params['subsample'],\n",
    "        colsample_bytree=global_xgb_params['colsample_bytree'],\n",
    "        min_child_weight=global_xgb_params['min_child_weight'],\n",
    "        objective=weighted_mse(global_xgb_params['alpha']),\n",
    "        tree_method=global_xgb_params['tree_method'],\n",
    "    )\n",
    "    \n",
    "    global_model.fit(\n",
    "        X_train, Y_train_t,\n",
    "        early_stopping_rounds=100,\n",
    "        eval_metric=custom_smape,\n",
    "        eval_set=evals,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦ ì˜ˆì¸¡\n",
    "    global_pred_t = global_model.predict(X_valid)\n",
    "    global_pred_val = np.expm1(global_pred_t)\n",
    "    global_pred.loc[Y_valid.index, 'pred'] = global_pred_val\n",
    "    \n",
    "    # SMAPE ê³„ì‚°\n",
    "    global_smape = smape(Y_valid.values, global_pred_val)\n",
    "    global_fold_smape.append(global_smape)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "    global_answer_t = global_model.predict(X_test_global)\n",
    "    global_answer_list.append(np.expm1(global_answer_t))\n",
    "\n",
    "# ê¸€ë¡œë²Œ ëª¨ë¸ ê²°ê³¼\n",
    "global_type_answer = sum(global_answer_list) / len(global_answer_list)\n",
    "global_answer = pd.DataFrame({'answer': global_type_answer}, index=X_test_global.index)\n",
    "global_answer_df = pd.concat([global_answer_df, global_answer], axis=0)\n",
    "global_pred_df = pd.concat([global_pred_df, global_pred], axis=0)\n",
    "\n",
    "global_avg_smape = sum(global_fold_smape) / len(global_fold_smape)\n",
    "print(f'ê¸€ë¡œë²Œ ëª¨ë¸ SMAPE : {global_avg_smape:.4f}')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2ë‹¨ê³„: ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ í•™ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== íƒ€ì…ë³„ ëª¨ë¸ í•™ìŠµ (ê¸°ì¡´ ì½”ë“œ) =====\n",
    "for i in type_list:\n",
    "    print(f\"\\nê±´ë¬¼ íƒ€ì…: {i}\")\n",
    "    \n",
    "    # ë°ì´í„° ì¤€ë¹„\n",
    "    x = X[X.building_type == i].copy()\n",
    "    y = Y[Y.building_type == i]['power_consumption'].astype(float).copy()\n",
    "    X_test_i = test_X[test_X.building_type == i].copy()\n",
    "\n",
    "    # ë¼ë²¨ í´ë¦°ì—…\n",
    "    y = y.replace([np.inf, -np.inf], np.nan)\n",
    "    ok = y.notna() & (y >= 0)\n",
    "    if not ok.all():\n",
    "        x = x.loc[ok]\n",
    "        y = y.loc[ok]\n",
    "\n",
    "    # ì›í•« ì¸ì½”ë”©\n",
    "    x = pd.get_dummies(x, columns=['building_number'], drop_first=False)\n",
    "    X_test_i = pd.get_dummies(X_test_i, columns=['building_number'], drop_first=False)\n",
    "\n",
    "    # ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°\n",
    "    for df_ in (x, X_test_i):\n",
    "        if 'building_type' in df_.columns:\n",
    "            df_.drop(columns=['building_type'], inplace=True)\n",
    "\n",
    "    # train/test ë”ë¯¸ ì»¬ëŸ¼ ì •ë ¬\n",
    "    x, X_test_i = x.align(X_test_i, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "    # NaN/Inf ì²˜ë¦¬\n",
    "    x = x.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    X_test_i = X_test_i.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    # ì»¬ëŸ¼ëª… ì €ì¥\n",
    "    x_columns = np.array(x.columns)\n",
    "\n",
    "    # ë°ì´í„° íƒ€ì… ì •ë¦¬\n",
    "    x = x.astype(np.float32)\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    j = 0\n",
    "    xgb_fold_smape = []\n",
    "    answer_list = []\n",
    "\n",
    "    # OOF ê·¸ë¦‡ ìƒì„±\n",
    "    pred = pd.DataFrame(index=y.index.copy(), columns=['pred'])\n",
    "\n",
    "    for train_index, valid_index in kf.split(x):\n",
    "        j += 1\n",
    "\n",
    "        X_train, X_valid = x.iloc[train_index], x.iloc[valid_index]\n",
    "        Y_train, Y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        # log1p ë³€í™˜\n",
    "        Y_train_t = np.log1p(np.clip(Y_train, a_min=0, a_max=None))\n",
    "        Y_valid_t = np.log1p(np.clip(Y_valid, a_min=0, a_max=None))\n",
    "\n",
    "        evals = [(X_train, Y_train_t), (X_valid, Y_valid_t)]\n",
    "\n",
    "        xgb_model = XGBRegressor(\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=5000,\n",
    "            max_depth=int(xgb_best_params.loc[i]['max_depth']),\n",
    "            random_state=RANDOM_SEED,\n",
    "            subsample=xgb_best_params.loc[i]['subsample'],\n",
    "            colsample_bytree=xgb_best_params.loc[i]['colsample_bytree'],\n",
    "            min_child_weight=int(xgb_best_params.loc[i]['min_child_weight']),\n",
    "            objective=weighted_mse(xgb_best_params.loc[i]['alpha']),\n",
    "            tree_method='gpu_hist',\n",
    "        )\n",
    "\n",
    "        xgb_model.fit(\n",
    "            X_train, Y_train_t,\n",
    "            early_stopping_rounds=100,\n",
    "            eval_metric=custom_smape,\n",
    "            eval_set=evals,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # ê²€ì¦ ì˜ˆì¸¡\n",
    "        xgb_pred_t = xgb_model.predict(X_valid)\n",
    "        xgb_pred = np.expm1(xgb_pred_t)\n",
    "        pred.loc[Y_valid.index, 'pred'] = xgb_pred\n",
    "\n",
    "        # SMAPE ê³„ì‚°\n",
    "        xgb_smape = smape(Y_valid.values, xgb_pred)\n",
    "        xgb_fold_smape.append(xgb_smape)\n",
    "\n",
    "        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "        xgb_answer_t = xgb_model.predict(X_test_i)\n",
    "        answer_list.append(np.expm1(xgb_answer_t))\n",
    "\n",
    "        # ë§ˆì§€ë§‰ í´ë“œì—ì„œ ì¤‘ìš”ë„ ê·¸ë¦¬ê¸°\n",
    "        if j == kf.n_splits:\n",
    "            sorted_idx = xgb_model.feature_importances_.argsort()\n",
    "            plt.figure(figsize=(8, 15))\n",
    "            plt.barh(x_columns[sorted_idx], xgb_model.feature_importances_[sorted_idx])\n",
    "            plt.xlabel(f'{i} model XGB Feature Importance')\n",
    "            plt.show()\n",
    "\n",
    "    # íƒ€ì…ë³„ í…ŒìŠ¤íŠ¸ í‰ê· \n",
    "    type_answer = sum(answer_list) / len(answer_list)\n",
    "    answer = pd.DataFrame({'answer': type_answer}, index=X_test_i.index)\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    answer_df = pd.concat([answer_df, answer], axis=0)\n",
    "    pred_df = pd.concat([pred_df, pred], axis=0)\n",
    "\n",
    "    avg_smape = sum(xgb_fold_smape) / len(xgb_fold_smape)\n",
    "    print(f'Building type = {i} : XGBRegressor Model SMAPE : {avg_smape:.4f}')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3ë‹¨ê³„: ì•™ìƒë¸” ê²°ê³¼ ê³„ì‚°\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== ì•™ìƒë¸” ê³„ì‚° =====\n",
    "# ê²€ì¦ ë°ì´í„° ì•™ìƒë¸” (SMAPE ê³„ì‚°ìš©)\n",
    "ensemble_pred_df = pd.DataFrame(index=pred_df.index, columns=['pred'])\n",
    "for idx in pred_df.index:\n",
    "    if idx in global_pred_df.index:\n",
    "        type_pred = pred_df.loc[idx, 'pred']\n",
    "        global_pred_val = global_pred_df.loc[idx, 'pred']\n",
    "        ensemble_pred = (ENSEMBLE_WEIGHT_TYPE * type_pred + \n",
    "                        ENSEMBLE_WEIGHT_GLOBAL * global_pred_val)\n",
    "        ensemble_pred_df.loc[idx, 'pred'] = ensemble_pred\n",
    "    else:\n",
    "        ensemble_pred_df.loc[idx, 'pred'] = pred_df.loc[idx, 'pred']\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì•™ìƒë¸”\n",
    "ensemble_answer_df = pd.DataFrame(index=answer_df.index, columns=['answer'])\n",
    "for idx in answer_df.index:\n",
    "    if idx in global_answer_df.index:\n",
    "        type_answer = answer_df.loc[idx, 'answer']\n",
    "        global_answer_val = global_answer_df.loc[idx, 'answer']\n",
    "        ensemble_answer = (ENSEMBLE_WEIGHT_TYPE * type_answer + \n",
    "                          ENSEMBLE_WEIGHT_GLOBAL * global_answer_val)\n",
    "        ensemble_answer_df.loc[idx, 'answer'] = ensemble_answer\n",
    "    else:\n",
    "        ensemble_answer_df.loc[idx, 'answer'] = answer_df.loc[idx, 'answer']\n",
    "\n",
    "# SMAPE ê³„ì‚°\n",
    "type_only_score = smape(\n",
    "    Y.loc[pred_df.index, 'power_consumption'].values.astype(float),\n",
    "    pred_df['pred'].values.astype(float)\n",
    ")\n",
    "\n",
    "ensemble_score = smape(\n",
    "    Y.loc[ensemble_pred_df.index, 'power_consumption'].values.astype(float),\n",
    "    ensemble_pred_df['pred'].values.astype(float)\n",
    ")\n",
    "\n",
    "print(f'íƒ€ì…ë³„ ëª¨ë¸ë§Œ ì‚¬ìš©í•œ Total SMAPE : {type_only_score:.4f}')\n",
    "print(f'ê¸€ë¡œë²Œ ëª¨ë¸ë§Œ ì‚¬ìš©í•œ Total SMAPE : {global_avg_smape:.4f}')\n",
    "print(f'ì•™ìƒë¸” ëª¨ë¸ Total SMAPE : {ensemble_score:.4f}')\n",
    "print(f'ì„±ëŠ¥ ê°œì„  : {type_only_score - ensemble_score:.4f}')\n",
    "\n",
    "# ê¸°ì¡´ í˜•ì‹ê³¼ ë™ì¼í•˜ê²Œ ì¶œë ¥\n",
    "print('Total SMAPE : %.4f' % (ensemble_score))\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ë¥¼ ensemble_answer_dfë¡œ ì„¤ì •\n",
    "answer_df = ensemble_answer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c4abc",
   "metadata": {},
   "source": [
    "## ì •ë‹µíŒŒì¼ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef13fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T11:44:33.046251Z",
     "start_time": "2023-08-30T11:44:33.033239Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959ba4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T11:44:33.226417Z",
     "start_time": "2023-08-30T11:44:33.210401Z"
    }
   },
   "outputs": [],
   "source": [
    "answer.answer = answer_df.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcff61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T11:44:33.676826Z",
     "start_time": "2023-08-30T11:44:33.653804Z"
    }
   },
   "outputs": [],
   "source": [
    "answer.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b51d62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T12:18:33.109076Z",
     "start_time": "2023-09-01T12:18:33.095063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9974b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
