{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f95f31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3의 최적 파라미터가 'best_params.pkl'에 저장되었습니다.\n",
      "저장된 파라미터 개수: 59\n",
      "\n",
      "저장된 파라미터 확인:\n",
      "✓ 파라미터 로딩 성공\n",
      "Global learning rate: 0.055273\n",
      "Global n_estimators: 5822\n",
      "Ensemble weight type: 0.525733\n",
      "CDH threshold: 20\n"
     ]
    }
   ],
   "source": [
    "# Trial 3의 최적 파라미터를 직접 저장하는 코드\n",
    "import pickle\n",
    "\n",
    "# 최적 파라미터 딕셔너리\n",
    "best_params_trial3 = {\n",
    "    'global_learning_rate': 0.05527306870639684,\n",
    "    'global_n_estimators': 5822,\n",
    "    'global_max_depth': 9,\n",
    "    'global_subsample': 0.9544679438650066,\n",
    "    'global_colsample_bytree': 0.8286782238457977,\n",
    "    'global_min_child_weight': 8,\n",
    "    'global_alpha': 2.9896290698185526,\n",
    "    'ensemble_weight_type': 0.5257330598296284,\n",
    "    'cdh_threshold': 20,\n",
    "    'Hotel and Resort_max_depth': 4,\n",
    "    'Hotel and Resort_subsample': 0.6710304947272696,\n",
    "    'Hotel and Resort_colsample_bytree': 0.7225819434668705,\n",
    "    'Hotel and Resort_min_child_weight': 7,\n",
    "    'Hotel and Resort_alpha': 1.8559664363552197,\n",
    "    'Commercial_max_depth': 12,\n",
    "    'Commercial_subsample': 0.6392220434384972,\n",
    "    'Commercial_colsample_bytree': 0.9515053077101255,\n",
    "    'Commercial_min_child_weight': 1,\n",
    "    'Commercial_alpha': 1.7726391394042025,\n",
    "    'Hospital_max_depth': 12,\n",
    "    'Hospital_subsample': 0.7834130786932383,\n",
    "    'Hospital_colsample_bytree': 0.8747419906952096,\n",
    "    'Hospital_min_child_weight': 10,\n",
    "    'Hospital_alpha': 2.6696350178171677,\n",
    "    'University_max_depth': 8,\n",
    "    'University_subsample': 0.6486487772052955,\n",
    "    'University_colsample_bytree': 0.9041585700584077,\n",
    "    'University_min_child_weight': 4,\n",
    "    'University_alpha': 1.6252188158037522,\n",
    "    'Other Buildings_max_depth': 7,\n",
    "    'Other Buildings_subsample': 0.8533171422821397,\n",
    "    'Other Buildings_colsample_bytree': 0.7660582811424111,\n",
    "    'Other Buildings_min_child_weight': 8,\n",
    "    'Other Buildings_alpha': 1.7338394580293282,\n",
    "    'Apartment_max_depth': 8,\n",
    "    'Apartment_subsample': 0.7081977526024632,\n",
    "    'Apartment_colsample_bytree': 0.909144720152893,\n",
    "    'Apartment_min_child_weight': 5,\n",
    "    'Apartment_alpha': 2.500036497364502,\n",
    "    'Research Institute_max_depth': 4,\n",
    "    'Research Institute_subsample': 0.84354910638235,\n",
    "    'Research Institute_colsample_bytree': 0.8233897593306069,\n",
    "    'Research Institute_min_child_weight': 6,\n",
    "    'Research Institute_alpha': 2.779822733140019,\n",
    "    'Department Store and Outlet_max_depth': 7,\n",
    "    'Department Store and Outlet_subsample': 0.9688870153034745,\n",
    "    'Department Store and Outlet_colsample_bytree': 0.6012249969270405,\n",
    "    'Department Store and Outlet_min_child_weight': 9,\n",
    "    'Department Store and Outlet_alpha': 2.92217446971382,\n",
    "    'Data Center_max_depth': 5,\n",
    "    'Data Center_subsample': 0.8827607655229703,\n",
    "    'Data Center_colsample_bytree': 0.6658627945301703,\n",
    "    'Data Center_min_child_weight': 5,\n",
    "    'Data Center_alpha': 1.6775240395880826,\n",
    "    'Public_max_depth': 6,\n",
    "    'Public_subsample': 0.6043607703917003,\n",
    "    'Public_colsample_bytree': 0.8508950613484505,\n",
    "    'Public_min_child_weight': 6,\n",
    "    'Public_alpha': 1.3198797950118193\n",
    "}\n",
    "\n",
    "# 최적 파라미터를 파일로 저장\n",
    "def save_trial3_params():\n",
    "    \"\"\"Trial 3의 최적 파라미터를 저장\"\"\"\n",
    "    with open('best_params.pkl', 'wb') as f:\n",
    "        pickle.dump(best_params_trial3, f)\n",
    "    print(\"Trial 3의 최적 파라미터가 'best_params.pkl'에 저장되었습니다.\")\n",
    "    print(f\"저장된 파라미터 개수: {len(best_params_trial3)}\")\n",
    "\n",
    "# 저장 실행\n",
    "save_trial3_params()\n",
    "\n",
    "# 저장이 제대로 되었는지 확인\n",
    "print(\"\\n저장된 파라미터 확인:\")\n",
    "loaded_params = None\n",
    "try:\n",
    "    with open('best_params.pkl', 'rb') as f:\n",
    "        loaded_params = pickle.load(f)\n",
    "    print(\"✓ 파라미터 로딩 성공\")\n",
    "    print(f\"Global learning rate: {loaded_params['global_learning_rate']:.6f}\")\n",
    "    print(f\"Global n_estimators: {loaded_params['global_n_estimators']}\")\n",
    "    print(f\"Ensemble weight type: {loaded_params['ensemble_weight_type']:.6f}\")\n",
    "    print(f\"CDH threshold: {loaded_params['cdh_threshold']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ 파라미터 로딩 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d588ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna 최적화 목적 함수\"\"\"\n",
    "    \n",
    "    # 글로벌 모델 파라미터 최적화\n",
    "    global_params = {\n",
    "        'learning_rate': trial.suggest_float('global_learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': trial.suggest_int('global_n_estimators', 1000, 8000),\n",
    "        'max_depth': trial.suggest_int('global_max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('global_subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('global_colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('global_min_child_weight', 1, 10),\n",
    "        'alpha': trial.suggest_float('global_alpha', 1.0, 3.0),\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "    \n",
    "    # 앙상블 가중치 최적화\n",
    "    ensemble_weight_type = trial.suggest_float('ensemble_weight_type', 0.3, 0.9)\n",
    "    ensemble_weight_global = 1.0 - ensemble_weight_type\n",
    "    \n",
    "    # CDH 파라미터 최적화\n",
    "    cdh_threshold = trial.suggest_int('cdh_threshold', 20, 30)\n",
    "    \n",
    "    # 타입별 모델 파라미터 최적화 (각 건물 타입별로)\n",
    "    type_params = {}\n",
    "    for building_type in type_list:\n",
    "        type_params[building_type] = {\n",
    "            'max_depth': trial.suggest_int(f'{building_type}_max_depth', 3, 12),\n",
    "            'subsample': trial.suggest_float(f'{building_type}_subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float(f'{building_type}_colsample_bytree', 0.6, 1.0),\n",
    "            'min_child_weight': trial.suggest_int(f'{building_type}_min_child_weight', 1, 10),\n",
    "            'alpha': trial.suggest_float(f'{building_type}_alpha', 1.0, 3.0)\n",
    "        }\n",
    "    \n",
    "    # 모델 학습 및 평가 (기존 로직과 동일하되 파라미터만 변경)\n",
    "    kf = KFold(n_splits=7, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # 글로벌 모델 학습\n",
    "    X_global = X.copy()\n",
    "    Y_global = Y['power_consumption'].astype(float).copy()\n",
    "    X_test_global = test_X.copy()\n",
    "    \n",
    "    Y_global = Y_global.replace([np.inf, -np.inf], np.nan)\n",
    "    ok_global = Y_global.notna() & (Y_global >= 0)\n",
    "    if not ok_global.all():\n",
    "        X_global = X_global.loc[ok_global]\n",
    "        Y_global = Y_global.loc[ok_global]\n",
    "    \n",
    "    X_global = pd.get_dummies(X_global, columns=['building_number', 'building_type'], drop_first=False)\n",
    "    X_test_global = pd.get_dummies(X_test_global, columns=['building_number', 'building_type'], drop_first=False)\n",
    "    X_global, X_test_global = X_global.align(X_test_global, join='outer', axis=1, fill_value=0)\n",
    "    X_global = X_global.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    X_test_global = X_test_global.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    X_global = X_global.astype(np.float32)\n",
    "    Y_global = Y_global.astype(np.float32)\n",
    "    \n",
    "    global_fold_smape = []\n",
    "    global_pred = pd.DataFrame(index=Y_global.index.copy(), columns=['pred'])\n",
    "    \n",
    "    for train_index, valid_index in kf.split(X_global):\n",
    "        X_train, X_valid = X_global.iloc[train_index], X_global.iloc[valid_index]\n",
    "        Y_train, Y_valid = Y_global.iloc[train_index], Y_global.iloc[valid_index]\n",
    "        \n",
    "        Y_train_t = np.log1p(np.clip(Y_train, a_min=0, a_max=None))\n",
    "        Y_valid_t = np.log1p(np.clip(Y_valid, a_min=0, a_max=None))\n",
    "        \n",
    "        evals = [(X_train, Y_train_t), (X_valid, Y_valid_t)]\n",
    "        \n",
    "        global_model = XGBRegressor(\n",
    "            learning_rate=global_params['learning_rate'],\n",
    "            n_estimators=global_params['n_estimators'],\n",
    "            max_depth=global_params['max_depth'],\n",
    "            random_state=global_params['random_state'],\n",
    "            subsample=global_params['subsample'],\n",
    "            colsample_bytree=global_params['colsample_bytree'],\n",
    "            min_child_weight=global_params['min_child_weight'],\n",
    "            objective=weighted_mse(global_params['alpha']),\n",
    "            tree_method=global_params['tree_method'],\n",
    "        )\n",
    "        \n",
    "        global_model.fit(\n",
    "            X_train, Y_train_t,\n",
    "            early_stopping_rounds=100,\n",
    "            eval_metric=custom_smape,\n",
    "            eval_set=evals,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        global_pred_t = global_model.predict(X_valid)\n",
    "        global_pred_val = np.expm1(global_pred_t)\n",
    "        global_pred.loc[Y_valid.index, 'pred'] = global_pred_val\n",
    "        \n",
    "        global_smape = smape(Y_valid.values, global_pred_val)\n",
    "        global_fold_smape.append(global_smape)\n",
    "    \n",
    "    # 타입별 모델 학습\n",
    "    type_pred_df = pd.DataFrame(columns=['pred'])\n",
    "    \n",
    "    for i in type_list:\n",
    "        x = X[X.building_type == i].copy()\n",
    "        y = Y[Y.building_type == i]['power_consumption'].astype(float).copy()\n",
    "        \n",
    "        y = y.replace([np.inf, -np.inf], np.nan)\n",
    "        ok = y.notna() & (y >= 0)\n",
    "        if not ok.all():\n",
    "            x = x.loc[ok]\n",
    "            y = y.loc[ok]\n",
    "        \n",
    "        x = pd.get_dummies(x, columns=['building_number'], drop_first=False)\n",
    "        \n",
    "        for df_ in (x,):\n",
    "            if 'building_type' in df_.columns:\n",
    "                df_.drop(columns=['building_type'], inplace=True)\n",
    "        \n",
    "        x = x.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        x = x.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        \n",
    "        type_fold_smape = []\n",
    "        pred = pd.DataFrame(index=y.index.copy(), columns=['pred'])\n",
    "        \n",
    "        for train_index, valid_index in kf.split(x):\n",
    "            X_train, X_valid = x.iloc[train_index], x.iloc[valid_index]\n",
    "            Y_train, Y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "            Y_train_t = np.log1p(np.clip(Y_train, a_min=0, a_max=None))\n",
    "            Y_valid_t = np.log1p(np.clip(Y_valid, a_min=0, a_max=None))\n",
    "            \n",
    "            evals = [(X_train, Y_train_t), (X_valid, Y_valid_t)]\n",
    "            \n",
    "            xgb_model = XGBRegressor(\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=5000,\n",
    "                max_depth=type_params[i]['max_depth'],\n",
    "                random_state=RANDOM_SEED,\n",
    "                subsample=type_params[i]['subsample'],\n",
    "                colsample_bytree=type_params[i]['colsample_bytree'],\n",
    "                min_child_weight=type_params[i]['min_child_weight'],\n",
    "                objective=weighted_mse(type_params[i]['alpha']),\n",
    "                tree_method='gpu_hist',\n",
    "            )\n",
    "            \n",
    "            xgb_model.fit(\n",
    "                X_train, Y_train_t,\n",
    "                early_stopping_rounds=100,\n",
    "                eval_metric=custom_smape,\n",
    "                eval_set=evals,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            xgb_pred_t = xgb_model.predict(X_valid)\n",
    "            xgb_pred = np.expm1(xgb_pred_t)\n",
    "            pred.loc[Y_valid.index, 'pred'] = xgb_pred\n",
    "            \n",
    "            xgb_smape = smape(Y_valid.values, xgb_pred)\n",
    "            type_fold_smape.append(xgb_smape)\n",
    "        \n",
    "        type_pred_df = pd.concat([type_pred_df, pred], axis=0)\n",
    "    \n",
    "    # 앙상블 계산\n",
    "    ensemble_pred_df = pd.DataFrame(index=type_pred_df.index, columns=['pred'])\n",
    "    for idx in type_pred_df.index:\n",
    "        if idx in global_pred.index:\n",
    "            type_pred = type_pred_df.loc[idx, 'pred']\n",
    "            global_pred_val = global_pred.loc[idx, 'pred']\n",
    "            ensemble_pred = (ensemble_weight_type * type_pred + \n",
    "                            ensemble_weight_global * global_pred_val)\n",
    "            ensemble_pred_df.loc[idx, 'pred'] = ensemble_pred\n",
    "        else:\n",
    "            ensemble_pred_df.loc[idx, 'pred'] = type_pred_df.loc[idx, 'pred']\n",
    "    \n",
    "    # 최종 SMAPE 계산\n",
    "    ensemble_score = smape(\n",
    "        Y.loc[ensemble_pred_df.index, 'power_consumption'].values.astype(float),\n",
    "        ensemble_pred_df['pred'].values.astype(float)\n",
    "    )\n",
    "    \n",
    "    return ensemble_score\n",
    "\n",
    "def optimize_parameters(n_trials=100):\n",
    "    \"\"\"파라미터 최적화 실행\"\"\"\n",
    "    \n",
    "    # Optuna study 생성\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    # 최적화 실행\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=None)\n",
    "    \n",
    "    # 최적 파라미터 저장\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # 파라미터를 파일로 저장\n",
    "    with open('best_params.pkl', 'wb') as f:\n",
    "        pickle.dump(best_params, f)\n",
    "    \n",
    "    print(f\"Best SMAPE: {study.best_value:.4f}\")\n",
    "    print(\"Best parameters saved to 'best_params.pkl'\")\n",
    "    \n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193251c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 최적화 실행 (시간이 오래 걸리므로 필요할 때만 실행)\n",
    "optimize_parameters(n_trials=100)  # 주석 해제하여 실행\n",
    "\n",
    "# 이미 최적화가 완료되었다면 아래로 파라미터 확인\n",
    "best_params = load_best_params()\n",
    "if best_params:\n",
    "    print(\"현재 저장된 최적 파라미터:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f2c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
